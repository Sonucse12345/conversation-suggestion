{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11107104,"sourceType":"datasetVersion","datasetId":6924528},{"sourceId":11137662,"sourceType":"datasetVersion","datasetId":6946998}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:13.550115Z","iopub.execute_input":"2025-04-11T11:41:13.550502Z","iopub.status.idle":"2025-04-11T11:41:13.570219Z","shell.execute_reply.started":"2025-04-11T11:41:13.550446Z","shell.execute_reply":"2025-04-11T11:41:13.569244Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/newdating/newdating.txt\n/kaggle/input/cleaned-text/cleaned_text.txt\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"pip install langchain_community","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:13.571596Z","iopub.execute_input":"2025-04-11T11:41:13.571865Z","iopub.status.idle":"2025-04-11T11:41:24.049289Z","shell.execute_reply.started":"2025-04-11T11:41:13.571842Z","shell.execute_reply":"2025-04-11T11:41:24.048084Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: langchain_community in /usr/local/lib/python3.10/dist-packages (0.3.21)\nCollecting langchain-core<1.0.0,>=0.3.51 (from langchain_community)\n  Using cached langchain_core-0.3.51-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain<1.0.0,>=0.3.23 (from langchain_community)\n  Using cached langchain-0.3.23-py3-none-any.whl.metadata (7.8 kB)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.0.36)\nRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.32.3)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (6.0.2)\nRequirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (3.11.12)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (8.5.0)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.6.7)\nRequirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (2.8.1)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.1.147)\nRequirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (0.4.0)\nRequirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from langchain_community) (1.26.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (4.0.3)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\nCollecting langchain-text-splitters<1.0.0,>=0.3.8 (from langchain<1.0.0,>=0.3.23->langchain_community)\n  Using cached langchain_text_splitters-0.3.8-py3-none-any.whl.metadata (1.9 kB)\nCollecting pydantic<3.0.0,>=2.7.4 (from langchain<1.0.0,>=0.3.23->langchain_community)\n  Downloading pydantic-2.11.3-py3-none-any.whl.metadata (65 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.2/65.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (1.33)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain_community) (4.12.2)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3,>=1.26.2->langchain_community) (2.4.1)\nRequirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\nRequirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain_community) (3.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community) (0.7.0)\nCollecting pydantic-core==2.33.1 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community)\n  Downloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\nCollecting typing-inspection>=0.4.0 (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.23->langchain_community)\n  Downloading typing_inspection-0.4.0-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3,>=1.26.2->langchain_community) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3,>=1.26.2->langchain_community) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3,>=1.26.2->langchain_community) (2024.2.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.2.2)\nUsing cached langchain-0.3.23-py3-none-any.whl (1.0 MB)\nUsing cached langchain_core-0.3.51-py3-none-any.whl (423 kB)\nUsing cached langchain_text_splitters-0.3.8-py3-none-any.whl (32 kB)\nDownloading pydantic-2.11.3-py3-none-any.whl (443 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m443.6/443.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pydantic_core-2.33.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading typing_inspection-0.4.0-py3-none-any.whl (14 kB)\nInstalling collected packages: typing-inspection, pydantic-core, pydantic, langchain-core, langchain-text-splitters, langchain\n  Attempting uninstall: pydantic-core\n    Found existing installation: pydantic_core 2.29.0\n    Uninstalling pydantic_core-2.29.0:\n      Successfully uninstalled pydantic_core-2.29.0\n  Attempting uninstall: pydantic\n    Found existing installation: pydantic 1.10.21\n    Uninstalling pydantic-1.10.21:\n      Successfully uninstalled pydantic-1.10.21\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.2.43\n    Uninstalling langchain-core-0.2.43:\n      Successfully uninstalled langchain-core-0.2.43\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.2.4\n    Uninstalling langchain-text-splitters-0.2.4:\n      Successfully uninstalled langchain-text-splitters-0.2.4\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.2.15\n    Uninstalling langchain-0.2.15:\n      Successfully uninstalled langchain-0.2.15\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nlangchain-pinecone 0.1.2 requires pinecone-client<5,>=3.2.2, which is not installed.\npinecone-datasets 0.7.0 requires pinecone-client<4.0.0,>=3.0.0, which is not installed.\nlangchain-openai 0.1.23 requires langchain-core<0.3.0,>=0.2.35, but you have langchain-core 0.3.51 which is incompatible.\nlangchain-pinecone 0.1.2 requires langchain-core<0.3,>=0.1.52, but you have langchain-core 0.3.51 which is incompatible.\npinecone-datasets 0.7.0 requires pydantic<2.0.0,>=1.10.5, but you have pydantic 2.11.3 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed langchain-0.3.23 langchain-core-0.3.51 langchain-text-splitters-0.3.8 pydantic-2.11.3 pydantic-core-2.33.1 typing-inspection-0.4.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"import re\nfrom langchain_community.document_loaders.text import TextLoader\n# Define the path to your text file\nfile_path = \"/kaggle/input/newdating/newdating.txt\"  # Replace with your file path\n# Initialize TextLoader with the file path\nloader = TextLoader(file_path)\n# Load the text data\ndata = loader.load()\n# Print the loaded text data\nprint(data[0].page_content[:1000])  # This will print the entire content of the text file","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:24.051333Z","iopub.execute_input":"2025-04-11T11:41:24.051658Z","iopub.status.idle":"2025-04-11T11:41:24.061408Z","shell.execute_reply.started":"2025-04-11T11:41:24.051627Z","shell.execute_reply":"2025-04-11T11:41:24.060549Z"}},"outputs":[{"name":"stdout","text":"Boy: It's been a while since we last talked. \nGirl: It has, but why? \nBoy: Are you in love again? \nGirl: No, I haven’t. I don’t want to. \nBoy: I get it, but why haven’t you? \nGirl: It’s just hard. I just wish… \nBoy: I wish that too. \nGirl: It’s been so long. \nBoy: We don’t have to lose time anymore. \nGirl: I’m afraid, what if we don’t. \nBoy: What if we do? \nGirl: What can we do then? \nBoy: We have to be patient, things are becoming. \nGirl: Things are becoming true again. \nBoy: I didn’t wish that to happen, I’m sorry. \nGirl: I’m sorry too. I planned my life to be with you, not without you and things seemed to be shattered. \nBoy: My heart is shattered, but I haven’t lost hope on us, I never stopped. \nGirl: I’d never stopped either. I think of you every once in a while, I think if you where could we be right now, if you were the one for me. If we were meant to be. \nBoy: It’s really hard seeing you not by my side, like we swear we would, and we both know I’m, we both know we are. \nGirl: No\n","output_type":"stream"}],"execution_count":57},{"cell_type":"code","source":"# Step 2: Define the function to clean text data\ndef replace_invalid_characters(text):\n    \"\"\"\n    Replace unwanted characters or text patterns in the input text.\n    This function will ensure that the dialogue remains intact without breaking words.\n    \"\"\"\n    # Remove unwanted characters (customize based on your data needs)\n    text = re.sub(r'[^a-zA-Z0-9\\s\\.:?!,♥\\(\\)-...]', '', text)  # Updated regex to allow \"...\"\n    # Normalize multiple spaces into one space and strip unwanted spaces at the start or end\n    text = re.sub(r'\\s+', ' ', text).strip()\n    # Ensure the dialogue remains in one continuous format without broken words\n    # Re-add proper spaces after each speaker label\n    text = re.sub(r'(Boy:|Girl:|Bf:|Gf:|Guy:)', r'\\n\\1 ', text)  # Add newline and ensure a single space after the speaker's name\n    return text\n\n# Step 8: Clean the text from your document (assuming `data[0].page_content` contains the raw text)\ndocument_text = data[0].page_content  # Assuming data[0] contains the first document's text\ncleaned_text = replace_invalid_characters(document_text)\nprint(cleaned_text[2:120])  # Prints the 6th character (indexing starts at 0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:24.062809Z","iopub.execute_input":"2025-04-11T11:41:24.063077Z","iopub.status.idle":"2025-04-11T11:41:24.087360Z","shell.execute_reply.started":"2025-04-11T11:41:24.063053Z","shell.execute_reply":"2025-04-11T11:41:24.085963Z"}},"outputs":[{"name":"stdout","text":"oy:  Its been a while since we last talked. \nGirl:  It has, but why? \nBoy:  Are you in love again? \nGirl:  No, I haven\n","output_type":"stream"}],"execution_count":58},{"cell_type":"markdown","source":"cleaned_text is string which cantain sequence of character of all boys and girl conversation","metadata":{}},{"cell_type":"code","source":"from langchain.document_loaders import DirectoryLoader\n# Save cleaned text to a file\ndirectory = \"/kaggle/working/\"\nos.makedirs(directory, exist_ok=True)\nfile_path = os.path.join(directory, \"cleaned_text.txt\")\n\nwith open(file_path, \"w\", encoding=\"utf-8\") as file:\n    file.write(cleaned_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:24.088588Z","iopub.execute_input":"2025-04-11T11:41:24.088918Z","iopub.status.idle":"2025-04-11T11:41:24.109291Z","shell.execute_reply.started":"2025-04-11T11:41:24.088891Z","shell.execute_reply":"2025-04-11T11:41:24.108257Z"}},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"from langchain.schema import Document\n# Load the cleaned text using TextLoader\nloader = TextLoader(file_path)\ndocuments = loader.load()\nfrom langchain.schema import Document\n\nprint(type(documents))        # Output: <class 'list'>\nprint(type(documents[0]))     # Output: <class 'langchain.schema.Document'>\nprint(documents[0].metadata)  # Output: {'source': '/kaggle/working/cleaned_text.txt'}\nprint(documents[0].page_content[:100])  # Output: The actual text content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:24.110356Z","iopub.execute_input":"2025-04-11T11:41:24.110650Z","iopub.status.idle":"2025-04-11T11:41:24.127233Z","shell.execute_reply.started":"2025-04-11T11:41:24.110625Z","shell.execute_reply":"2025-04-11T11:41:24.126222Z"}},"outputs":[{"name":"stdout","text":"<class 'list'>\n<class 'langchain_core.documents.base.Document'>\n{'source': '/kaggle/working/cleaned_text.txt'}\n\nBoy:  Its been a while since we last talked. \nGirl:  It has, but why? \nBoy:  Are you in love again?\n","output_type":"stream"}],"execution_count":60},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\n\ndef split_docs(documents,chunk_size=1000,chunk_overlap=50):\n  text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n  docs = text_splitter.split_documents(documents)\n  return docs\n\ndocs = split_docs(documents)\nprint(len(docs))\ntype(docs)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:24.130004Z","iopub.execute_input":"2025-04-11T11:41:24.130335Z","iopub.status.idle":"2025-04-11T11:41:24.148366Z","shell.execute_reply.started":"2025-04-11T11:41:24.130300Z","shell.execute_reply":"2025-04-11T11:41:24.147301Z"}},"outputs":[{"name":"stdout","text":"31\n","output_type":"stream"},{"execution_count":61,"output_type":"execute_result","data":{"text/plain":"list"},"metadata":{}}],"execution_count":61},{"cell_type":"code","source":"docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:24.150117Z","iopub.execute_input":"2025-04-11T11:41:24.150403Z","iopub.status.idle":"2025-04-11T11:41:24.172335Z","shell.execute_reply.started":"2025-04-11T11:41:24.150378Z","shell.execute_reply":"2025-04-11T11:41:24.171292Z"}},"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"[Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  Its been a while since we last talked. \\nGirl:  It has, but why? \\nBoy:  Are you in love again? \\nGirl:  No, I havent. I dont want to. \\nBoy:  I get it, but why havent you? \\nGirl:  Its just hard. I just wish \\nBoy:  I wish that too. \\nGirl:  Its been so long. \\nBoy:  We dont have to lose time anymore. \\nGirl:  Im afraid, what if we dont. \\nBoy:  What if we do? \\nGirl:  What can we do then? \\nBoy:  We have to be patient, things are becoming. \\nGirl:  Things are becoming true again. \\nBoy:  I didnt wish that to happen, Im sorry. \\nGirl:  Im sorry too. I planned my life to be with you, not without you and things seemed to be shattered. \\nBoy:  My heart is shattered, but I havent lost hope on us, I never stopped. \\nGirl:  Id never stopped either. I think of you every once in a while, I think if you where could we be right now, if you were the one for me. If we were meant to be. \\nBoy:  Its really hard seeing you not by my side, like we swear we would, and we both know Im, we both know we are.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  Now will never know. \\nBoy:  We know, but. \\nGirl:  But \\nBoy:  But what if we know, why we dont. \\nGirl:  Because I believe its too complicated, but we can. Just not right now. \\nBoy:  Ill wait for you if you tell me to. \\nGirl:  Time can be the only that can tell if we get our chance. \\nBoy:  Are you asking me to wait? \\nGirl:  Im, are you going to? \\nBoy:  Im. \\nGirl:  Im happy but I dont know if things are going to be like I think, like I dream them to be. Im scared to lose you, to lose us again. \\nBoy:  I promise things are going to be like you once dreamed them to be, I just wish to be in your dreams. \\nGirl:  You know you are in my dreams. \\nBoy:  Please can you look me in the eyes. \\nGirl:  Please say something. \\nBoy:  I love you. \\nGirl:  I love you too. \\nBoy:  Please dont be afraid Ill protect you, Ill make you happy, and we got a lifetime ahead just for you and me. \\nGirl:  I really really love you. \\nBoy:  I really love you too.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  I really love you too. \\nGirl:  I need you to be sincere and honest. Dont ever leave me again please. \\nBoy:  Babe, I never left you, Ive kept you in my heart since the day I first held your hand. \\nGirl:  I remember that day like if it was today. It felt really amazing, I couldnt say a word. I couldnt take my eyes from yours. \\nBoy:  It was really special for me, I fell in love in that second. I couldnt say a word either, didnt want to, because I was afraid of ruining our moment. \\nGirl:  Are those moments coming back? \\nBoy:  I hope someday they do. \\nGirl:  Me too. \\nBoy:  What do you want in life? \\nGirl:  Nobody had ever asked me that before. \\nBoy:  Do you have an idea? \\nGirl:  I just want to be happy. \\nBoy:  What will your happiness be made of? Am I included? \\nGirl:  I wish all of my dreams come true to be happy like in the fairytales. \\nBoy:  I can be your guardian angel, making you feel safe. \\nGirl:  I would really like that. \\nBoy:  I always saw you like no one ever had.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  I always saw you like no one ever had. \\nGirl:  Thats true. The way you see into my heart and soul, the way you know my emotions. Only you have that power over me. \\nBoy:  Really? \\nGirl:  I wouldnt lie to you about this. I never had done it before. \\nBoy:  I know you wouldnt, you are sacred for me. You mean the world to me. \\nGirl:  You mean everything to me, and if we get our chance I hope it can last forever. \\nBoy:  I promise it will. I wont be the same... Im going to do everything for you, whatever it takes to make you happy, thats what my heart tells me to do. \\nGirl:  I. \\nBoy:  You dont have to say something its okay. \\nGirl:  Thank you. \\nBoy:  Dont say thank you, I mean it. This is from my heart. I meant every single word I say to you. \\nGirl:  This feels like in the movies. \\nBoy:  So true so real. Is this pure? \\nGirl:  This is more than pure. \\nBoy:  Nothing is going to get between us. \\nGirl:  How do you know? \\nBoy:  I can see it in your eyes I can feel it in my heart.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  I feel it too. This is real. \\nBoy:  Ill put you first ahead of everything, Ill put us first ahead of everything. \\nGirl:  You dont have to promise everything. \\nBoy:  Okaywhen we get our chance if we do, I promise my words are going to get to action. Ill do as I say Im going to. \\nGirl:  I wish that we never be separate ever again. \\nBoy:  Please dont ever go. \\nGirl:  Please dont let me go. Please dont ever let me go. \\nBoy:  I wont, I wont. \\nGirl:  Can I tell you something? \\nBoy:  Yes please. \\nGirl:  Im afraid sometimes of getting my heart broken again. \\nBoy:  It is really my fault. Im truly sorry. Im going to make this up to you someday if you let me. \\nGirl:  I gave you my heart. What happened? \\nBoy:  I got lost. \\nGirl:  How could you do this to me? \\nBoy:  (silence). \\nGirl:  Why if you love me so much, why did you break my heart? \\nBoy:  I wont make any excuses. I got lost, I did it all wrong. \\nGirl:  (still crying) Im really in love with you. \\nBoy:  Let me hold you, please.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  Let me hold you, please. \\nGirl:  (didnt say anything). \\nBoy:  (holding her) I was just a boy. Id never stopped loving you since the day I met you. Id suffered since the day you left me. My heart got broken too, but I was too blind to see what I was doing to you. I was hurting you, and Im sorry for that. I love you with all my heart, because of you Im a better man, for you I wanted to be better so that maybe, just maybe, you could give us another chance. \\nGirl:  I know. I know you are being really sincere and honest with me. \\nBoy:  Im. \\nGirl:  (calming down) Im better because of you too. \\nBoy:  You make me see life from a different perspective. I found love in you. \\nGirl:  I found it in you. \\nBoy:  You still are the greatest thing that has happened in my life. \\nGirl:  (kept silence). \\nBoy:  You really are. \\nGirl:  Are you in love with me? \\nBoy:  Im deeply in love with you. \\nGirl:  But why? I know Im not perfect and that I have a lot of things that we argue about.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  Love is not easy. Its special, its happiness, sadness, its kind, joy, its sincere, you can feel a lot of emotions getting together when we are in love. I feel all of those emotions and more when Im with you. \\nGirl:  Thats really sweet. \\nBoy:  I mean it. \\nGirl:  I feel all those things for you, that is why Im so in love with you. Im really happy by your side. \\nBoy:  You make me feel complete again. It feels really nice. \\nGirl:  Im only complete with you. This time we were not together has been the toughest days ever. \\nBoy:  My days were endless and all I wanted is to sleep so that I couldnt think of you, but still sleeping you were on my mind. Every dream was of you. \\nGirl:  Is that bad? I thought that was okay. \\nBoy:  It is okay, but I know you were not with me, and knowing that you were not with me those thoughts really killed me. Those times when I wondered where you were, with whom, if you were happy or not, that made me feel worst.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  Ive felt exactly the same. Im just afraid of telling you all this without knowing what you felt. \\nBoy:  Dont be afraid, please, you are the only one who knows who I really am. \\nGirl:  I feel you know me too, like no other human had. \\nBoy:  You make me feel really special. \\nGirl:  You make me feel like if we have hope again. \\nBoy:  I never stopped wishing to be with you. \\nGirl:  I just wanted all of this loneliness to go away. I was too afraid. \\nBoy:  I know it took me a long time to let you know I still love you. \\nGirl:  Im glad you did. It doesnt matter how long you took. \\nBoy:  You are correct. What matters is that we were able to talk about this. \\nGirl:  Thats right. \\nBoy:  If we can only be together right now, it would make me happier. \\nGirl:  There is no need to rush things. Im not going anywhere without you, never again. \\nBoy:  This is gravity pulling us together again.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  This is gravity pulling us together again. \\nGirl:  This is our destiny, for us to be together. Thats what my life isus, you and me forever. It doesnt matter how hard this can be, I want to be with you. \\nBoy:  I see my future with you I see my life with you. I never thought a lifetime without you. \\nGirl:  But we are together now, lets take our time. \\nBoy:  Im never letting you go and if you go away someday Ill go with you. I love you. \\nGirl:  I love you too. \\nBoy:  Do you promise to be with me? \\nGirl:  I promise. Im giving us a second chance. \\nBoy:  Youll be happier than before. \\nGirl:  Id never felt this for someone. You make me feel so alive, so brave, so in love. \\nBoy:  Ill keep you safe Ill be everything and more for you. \\nGirl:  You already are. \\nBoy:  This is a new beginning, I waited for this so long. \\nGirl:  It is really happening, you and me all over again. Suddenly my fears are not getting in our way. \\nBoy:  Are you sure about this? We can wait if you want to.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  Im sure I want you in my life. I dont want to waste more time being without you. \\nBoy:  Well, Im here now for you. Ill be here forever. \\nGirl:  Im here too. \\nBoy:  (approaching a bit toward her) \\nGirl:  (approaching a bit toward him) \\nBoy:  (Looking at her eyes) \\nGirl:  (Looking at his eyes) \\nBoy:  May I kiss you? \\nGirl:  You may. \\nBoy:  All of this time we felt the same things. \\nGirl:  Were you afraid too? \\nBoy:  I was afraid of getting rejected by you. \\nGirl:  You felt vulnerable, right? \\nBoy:  I did. I didnt know how to deal with that. \\nGirl:  So you got scared. \\nBoy:  (looking nervous) Yes. \\nGirl:  I never thought you were afraid too. \\nBoy:  I was, but Im not anymore. Ive realized that by your side I have nothing to fear. \\nGirl:  When we are together there is nothing that I fear only the thought of us not being together scares me a lot. \\nBoy:  I dont want you to leave. \\nGirl:  I wont, I dont want to leave. \\nBoy:  We can definitely work this through if we want to.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  I know we can, I think we need this. \\nBoy:  I feel like if we were never separated. \\nGirl:  I feel the same way. \\nBoy:  Is there something I need to know? \\nGirl:  You were always on my mind. \\nBoy:  I like that. \\nGirl:  There is no day I stop thinking about you. \\nBoy:  We are still in your heart? \\nGirl:  We are. \\nBoy:  Never say never. Not with us. \\nGirl:  We can say forever. \\nBoy:  Forever? \\nGirl:  Yes, forever. Why do you ask? \\nBoy:  Because I have been waiting a long time to hear those words from you. \\nGirl:  Oh well we are going to. \\nBoy:  I wont sleep tonight thinking of today, about us, hoping that our lives that we can be together. \\nGirl:  You are really sweet. \\nBoy:  You are sweet with me. \\nGirl:  I think our chance is getting closer and closer. \\nBoy:  I cant wait. \\nGirl:  Me neither. \\nBoy:  Do you think we are going to get married? \\nGirl:  I hope one day we will. Until that day Im going to keep dreaming and wishing. \\nBoy:  You are the queen of my heart.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  You are the queen of my heart. \\nGirl:  (Blushing) Thank you, my prince charming. \\nBoy:  So what happens next? \\nGirl:  We are going to see each other a lot more. \\nBoy:  I hope this is for our best. \\nGirl:  I love you. There is no other man for me like you. \\nBoy:  (Blushing) Thank you. You are really special for me too. \\nGirl:  You mean everything to me. \\nBoy:  You mean everything to me too. Im lost without you, please stay with me. \\nGirl:  Im all yours. I was always yours. \\nBoy:  I need you so much in my life. \\nGirl:  You are still the one. \\nBoy:  You are the one I see in my dreams sharing my life with. \\nGirl:  Why did we take so long to talk again? I dont regret it. I just wish this happened before. \\nBoy:  Oh before I forget I brought you some chocolates. \\nGirl:  Thats very kind of you, thank you. \\nBoy:  Youre welcome, go ahead open them. \\nGirl:  You can still surprise me that still amaze me. \\nBoy:  I try my best to keep you surprised not to do the same things all over again.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  Youre doing a great job then. \\nBoy:  The grace that you have makes me fall in love with you more and more. \\nGirl:  I do? \\nBoy:  You are always so careful, so kind and lovely with me it really makes me fall for you. \\nGirl:  You never told me that before. \\nBoy:  I have, maybe you never realize that before until now. \\nGirl:  (Crying) Babies? \\nBoy:  Yes, babies. I want to have babies with you someday. And will name it after you if our baby is a girl. \\nGirl:  (Still Crying) We are going to name it after you if it is a boy. \\nBoy:  (Kissing her forehead) I dont want to live my life without you anymore. I want to spend the rest of my life with you, no more excuses or time to think. I dont want to think anymore, I just want to be with you, my love. \\nGirl:  (Looking deeply at him) Is this true? \\nBoy:  This is true, this is our fate. \\nGirl:  You and me. \\nBoy:  You and me, this time its going to be better, no one will get between us. \\nGirl:  No one will.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  No one will. \\nBoy:  I love you with all my heart, my babe. \\nGirl:  Im really happy. My heart is pumping so fast, I feel such a rush of blood through my head. Im in love. \\nBoy:  Mine too, its really exciting, us. I cant think clearly. But that wont stop me from loving you. \\nGirl:  (Laughing) Dont worry, my dear. \\nBoy:  I would like to kiss you. May I? \\nGirl:  I would really love to kiss you too, and yes, my dear, you may. Boy and \\nGirl:  (Kissing slowly). \\nBoy:  I love you, my love. \\nGirl:  I love you too, my dear. \\nBoy:  We came a long way... Right? \\nGirl:  Im sure we have, my dear. Did you love someone else? \\nBoy:  I liked someone but I never loved. \\nGirl:  All of this time I knew, but I knew you wanted me too. \\nBoy:  I panicked when you left me. \\nGirl:  So did I. \\nBoy:  Why, did you like someone else? \\nGirl:  No. Why did you like her? \\nBoy:  I was trying to forget you, instead I loved you more. \\nGirl:  Why were you trying to forget me?'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  Why were you trying to forget me? \\nBoy:  I never lost my hope, but I was suffering so much. I thought that was maybe my way out. \\nGirl:  But it wasnt. \\nBoy:  No, it was not. \\nGirl:  You didnt have to be afraid of loving me. \\nBoy:  I know I didnt. \\nGirl:  Okay. \\nBoy:  But. you never answer my calls. \\nGirl:  I was just upset with you. I never stopped loving you. \\nBoy:  Oh my. \\nGirl:  You are my true love. \\nBoy:  As you are mine. \\nGirl:  After a while, I knew that again. But I didnt know what to do. \\nBoy:  You needed to say three words to make this all better. \\nGirl:  One thing I love about you is that with you I felt safe. Everything was great. With you, I learned to trust myself, to believe in myself. \\nBoy:  What I love from you, you believed in me when no one else did. You are everything. \\nGirl:  A lot of things were mistaken but never us. \\nBoy:  We were our only trust, our hope, our faith, we were our love, and we were just you and me. \\nGirl:  We were everything.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  We were everything. \\nBoy:  For once, things were right, things felt right. \\nGirl:  We were the greatest couple ever. \\nBoy:  Are we going to hide our love? \\nGirl:  I dont want to hide us, not anymore. \\nBoy:  So dont. \\nGirl:  You are very patient with me. \\nBoy:  Because I love you, you are worth waiting for. \\nGirl:  You are worth too. \\nBoy:  Dont worry, together we can achieve whatever we want. Most importantly, we can make us again. \\nGirl:  You give me hope and faith with your words. You give me something to believe in. Something worth taking a risk. You give me a reason to live. Thats what we are. \\nBoy:  You gave your heart. \\nGirl:  Where were you? \\nBoy:  Ive been watching you, Ive been here. \\nGirl:  We are going on the same path again. \\nBoy:  Are you going to leave me? \\nGirl:  Im. Im really sorry. \\nBoy:  I swear Ill never be happy again. \\nGirl:  We can maybe just be friends. \\nBoy:  Maybe can just be friends. You promise me. \\nGirl:  I just wanted to know what you felt.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  I just wanted to know what you felt. \\nBoy:  (silence) \\nGirl:  And for that, I had to play with your heart. \\nBoy:  Why? \\nGirl:  Eventually, you would hurt me if I didnt. I really dont want to be hurt anymore. \\nBoy:  This is the worst thing you can do to me, to us. \\nGirl:  Why do you believe theres still an us? \\nBoy:  Because of what you feel for me. Does it mean something? \\nGirl:  Maybe a lie. \\nBoy:  Wow. \\nGirl:  I love you. \\nBoy:  Please dont say that. \\nGirl:  Its true, I do. \\nBoy:  So now you know. \\nGirl:  What? \\nBoy:  How I feel. \\nGirl:  So, what does it feel to be vulnerable? \\nBoy:  I feel discovered, unfolded. \\nGirl:  Im sorry. You know what you made me feel. \\nBoy:  I think. \\nGirl:  So? \\nBoy:  So? \\nGirl:  I have to leave you now. \\nBoy:  I still dont get it. \\nGirl:  What, my prince? \\nBoy:  Why do you want to hurt me? \\nGirl:  Mm. \\nBoy:  I know I dont deserve this. \\nGirl:  I know you dont. \\nBoy:  So why?'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  I know you dont. \\nBoy:  So why? \\nGirl:  Because if I dont, you can hurt me again. And Id rather hurt you first, even though I love you. \\nBoy:  Is this revenge for you? Are you proud to see me like this? \\nGirl:  No, I suffer too, hurting you. \\nBoy:  You got me lost. \\nGirl:  Oh. \\nBoy:  I thought you wanted to hurt me. \\nGirl:  I do. Just for you to realize something. \\nBoy:  You love me, dont you? \\nGirl:  I do. I think I love you a bit too much. \\nBoy:  Why is that? \\nGirl:  Love is meant to be happy, not sad, and as you make me feel so happy, you make me feel sad sometimes. \\nBoy:  Im sorry. I never meant to hurt you. \\nGirl:  I know it was not your intention, but somehow you hurt me really deeply. \\nBoy:  So what about us? \\nGirl:  As for us, Im so in love with you, but I cant do this. I dont want to get hurt again. Youll always be in my heart. I love you. Youre my guardian angel. \\nBoy:  I miss you. \\nGirl:  And so? \\nBoy:  I really did. \\nGirl:  OK. \\nBoy:  Im sorry. \\nGirl:  What for?'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  OK. \\nBoy:  Im sorry. \\nGirl:  What for? \\nBoy:  For ignoring your efforts to communicate with me. \\nGirl:  Its OK. I got used to it, then I got tired, so I stopped trying and started forgetting. \\nBoy:  I.tried to forget about you, you see. \\nGirl:  . \\nBoy:  Because it tore me apart that we can never be. \\nGirl:  Its OK. \\nBoy:  Why is it so OK? \\nGirl:  I got used to days hoping youd be back, but then you never did. I started facing reality and started to move on. \\nBoy:  Wait. am I too late? \\nGirl:  Too late for what? \\nBoy:  To court you? \\nGirl:  You know, Ive always wanted to hear that from you. Back then, a year ago. But... I got used to only wishing for it... then realized it would never happen, so I stopped hoping. \\nBoy:  Im really sorry, but dont worry, this time, I will make your wishes come true.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  Its my turn to say sorry. Time got to me. Youve broken my heart already. I cant risk experiencing that again. Thank you anyway. For communicating with me after a year of silence... \\nGirl:  I want to say something. \\nBoy:  Yes, darling? \\nGirl:  I have a question for you. \\nBoy:  Alright, ask me. \\nGirl:  What do you see when you look in my eyes? \\nBoy:  You honestly want to know? \\nGirl:  Yeah! \\nBoy:  My future. \\nGirl:  I hate the fact that you are taller than me. \\nBoy:  Trust me, there is an advantage in it. \\nGirl:  What? \\nBoy:  When I hug you, you can listen to my heart which beats only for you. \\nBoy:  At last, I can hardly wait! \\nGirl:  Do you want me to leave? \\nBoy:  No, dont even think about it! \\nGirl:  Do you love me? \\nBoy:  Of course, always! \\nGirl:  Have you ever cheated on me? \\nBoy:  No, why are you asking? \\nGirl:  Will you kiss me? \\nBoy:  Every chance I get. \\nGirl:  Will you slap me? \\nBoy:  Hell no, are you crazy?! \\nGirl:  Can I trust you? \\nBoy:  Yes! \\nGirl:  Darling!!'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  Yes! \\nGirl:  Darling!! \\nBoy:  Oh my God, are you okay? \\nBoy:  Sweetie!! Talk to me!! \\nGirl:  I. \\nBoy:  You what?? YOU WHAT? \\nGirl:  I have cancer and Im on life support. \\nGirl:  Theyre taking me off tonight. \\nBoy:  Why?? \\nGirl:  I wanted to tell you, but I couldnt. \\nBoy:  Why didnt you tell me? \\nGirl:  I didnt want to hurt you. \\nBoy:  You could never hurt me. \\nGirl:  I just wanted to see if you felt about me the same as I felt about you. \\nBoy:  ? \\nGirl:  I love you more than anything. I would give you the world in a heartbeat. I would die for you and take a bullet for you. \\nBoy:  (crying) \\nGirl:  Dont be sad. I love you and Ill always be here with you. \\nBoy:  Then why did you break up with me? \\nBoy:  Could you stop reading Twilight? Its so annoying. \\nGirl:  Haha, why? Hes just fictional, sweetheart. \\nBoy:  I know but still. \\nGirl:  What exactly are you afraid of? Him popping out of the book? HAHA!'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  Hmm... Lets say he does. and him and I both call out your name... what will you do? \\nBoy:  I need someone to talk to. \\nGirl:  Im always here for you. \\nBoy:  I know. \\nGirl:  Whats wrong? \\nBoy:  I like her so much. \\nGirl:  Talk to her. \\nBoy:  I dont know. She wont ever like me. \\nGirl:  Dont say that. youre amazing. \\nBoy:  I just want her to know how I feel. \\nGirl:  Then tell her. \\nBoy:  She wont like me. \\nGirl:  How do you know that? \\nBoy:  I can just tell her. \\nGirl:  Well, just tell her. \\nBoy:  What should I say? \\nGirl:  Tell her how much you like her. \\nBoy:  I tell her daily. \\nGirl:  What do you mean? \\nBoy:  Im always with her. I love her. \\nGirl:  I know how you feel. I have the same problem. but hell never like me. \\nBoy:  Wait. Who do you like? \\nGirl:  Oh, some boy. \\nBoy:  Oh, she wont like me either. \\nGirl:  She does. \\nBoy:  How do you know? \\nGirl:  Because who wouldnt like you? \\nBoy:  You. \\nGirl:  Youre wrong. I love you. \\nBoy:  I love you too.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  I love you too. \\nGirl:  So are you going to talk to her? \\nBoy:  I just did. \\nBoy:  Baby, we need to talk. \\nGirl:  Ricardo, what do you mean? \\nBoy:  Something has come up. \\nGirl:  What? Whats wrong? Is it bad? \\nBoy:  I dont want to hurt you, baby. \\nGirl:  (Thinks) Oh my God, I hope he doesnt break up with me.I love him so much. \\nBoy:  Baby, are you there? \\nGirl:  Yeah, Im here. What is so important? \\nBoy:  Im not sure if I should say it. \\nGirl:  Well, you already brought it up, so please just tell me. \\nBoy:  Im leaving... \\nGirl:  Baby, what are you talking about? I dont want you to leave me, I love you. \\nBoy:  Not like that, I mean Im moving far away. \\nGirl:  Why? All of your family lives over here. \\nBoy:  Well, my father is sending me away to a boarding school far away. \\nBoy:  Hey, hun! \\nGirl:  Hey. \\nBoy:  I missed you at school today. Why werent you there? \\nGirl:  Yeah, I had to go to the doctor. \\nBoy:  Oh really? Why? \\nGirl:  Oh, nothing. Just some annual shots, thats all.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  Oh. \\nGirl:  So what did we do in math today? \\nBoy:  You didnt miss anything that great, just a lot of notes. \\nGirl:  Ok, good. \\nBoy:  Yeah. \\nGirl:  Hey, I have a question... \\nBoy:  Ok, ask away. \\nGirl:  How much do you love me? \\nBoy:  You know I love you more than anything in this world. \\nGirl:  Yeah... \\nBoy:  Why did you ask? \\nGirl:  (silence) \\nBoy:  Is something wrong? \\nGirl:  No. Nothing at all. \\nGirl:  How much do you care about me? \\nBoy:  I would give you the world in a heartbeat if I could. \\nGirl:  You would? \\nBoy:  Yeah, of course I would. Sounding worried, is there something wrong? \\nGirl:  No, everythings fine. \\nBoy:  Are you sure? \\nGirl:  Yeah. \\nBoy:  Okay, I hope so. \\nGirl:  Would you die for me? \\nBoy:  I would take a bullet for you any day, hun. \\nGirl:  Really? \\nBoy:  Any day. Now, seriously, is there something wrong? \\nGirl:  No, Im fine. Youre fine. Were fine. Everyone and everything is fine. \\nBoy:  Okay. \\nGirl:  Well, I have to go. Ill see you tomorrow at school.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  Alright, bye. I LOVE YOU! \\nGirl:  Yeah, I love you too. Bye. \\nBoy:  Hey, have you seen my girlfriend today? \\nBoy:  Hey. \\nGirl:  Oh, hi. \\nBoy:  Why werent you at school today? \\nGirl:  Uh, I had another appointment with the doctor. \\nBoy:  Are you sick? \\nGirl:  Um, I have to go. My moms calling on the other line. \\nBoy:  Ill wait. \\nGirl:  It may take a while. Ill call you later. \\nBoy:  Alright, I love you. Very long pause \\nGirl:  (with a tear in her eye) Look, I think we should break up. \\nBoy:  What!? \\nGirl:  Its the best thing for us right now. \\nBoy:  Why?? \\nGirl:  I love you. THE GIRL DOESNT COME TO SCHOOL FOR 3 MORE WEEKS AND DOESNT ANSWER HER PHONE. \\nBoy:  Hey dude. Friend: Hey. \\nBoy:  Whats up? Friend: Nothing. Hey, have you talked to your ex lately? \\nBoy:  No. \\nBoy:  WAIT! NO! Boy goes to the hospital, and to room 646, building A, suite 3. The girl is lying in the hospital bed. \\nBoy:  Oh my God, are you okay?? \\nGirl:  . \\nBoy:  Sweetie! Talk to me!! \\nGirl:  I.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  . \\nBoy:  Sweetie! Talk to me!! \\nGirl:  I. \\nBoy:  You what?? YOU WHAT??? \\nGirl:  I have cancer and Im on life support. \\nBoy:  Breaks into tears. \\nGirl:  Theyre taking me off tonight. \\nBoy:  Why? \\nGirl:  I wanted to tell you, but I couldnt. \\nBoy:  Why didnt you tell me? \\nGirl:  I didnt want to hurt you. \\nBoy:  You could never hurt me. \\nGirl:  I just wanted to see if you felt about me the same as I felt about you. \\nBoy:  .? \\nGirl:  I love you more than anything. I would give you the world in a heartbeat. I would die for you and take a bullet for you. \\nBoy:  (crying) \\nGirl:  Dont be sad. I love you and Ill always be here with you. \\nBoy:  Then why did you break up with me? \\nGirl:  Hi there! \\nBoy:  Hi! Will you be my friend? \\nGirl:  Only if you promise not to hurt me. \\nBoy:  What? \\nGirl:  My sister was crying last night and said to never be friends with a boy who will hurt you. So will you promise not to hurt me? \\nBoy:  I could never hurt you. At a Wedding Seventeen Years Later'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  You kept your promise. \\nBoy:  What? \\nGirl:  The first time I met you I had you promise to never hurt me and you didnt. \\nBoy:  How could I ever hurt you? I love you. \\nBoy:  How are you, sweetheart? \\nGirl:  Not good : \\nBoy:  Why? What happened? \\nGirl:  Smiled. Hugged him and said I LOVE YOU. \\nBoy:  Whats happening here? Will you please tell me? \\nGirl:  I just love whenever you scold me thats why I ate ice cream so that you will scold me. \\nGirl:  Well, I gotta go... \\nBoy:  Ya, me too. (I hope you dont cry) \\nGirl:  Bye. (I still love you and miss you) \\nBoy:  See you later. (I will never stop loving you) \\nBoy:  At last, I can hardly wait! \\nGirl:  Do you want me to leave? \\nBoy:  No, dont even think about it! \\nGirl:  Do you love me? \\nBoy:  Of course, always! \\nGirl:  Have you ever cheated on me? \\nBoy:  No, why are you asking? \\nGirl:  Will you kiss me? \\nBoy:  Every chance I get. \\nGirl:  Will you slap me? \\nBoy:  Hell no, are you crazy?! \\nGirl:  Can I trust you? \\nBoy:  Yes!'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  Can I trust you? \\nBoy:  Yes! \\nGirl:  Darling!! \\nBoy:  Why are you crying? \\nGirl:  Im just so overwhelmed with everything right now. \\nBoy:  What can I do to help? \\nGirl:  Just be here with me, thats enough. \\nBoy:  I promise, wherever you need me, Ill be there. \\nGirl:  That means everything to me. \\nBoy:  Lets just sit together for a while, okay? \\nGirl:  Yes, Id like that very much. \\nBoy:  Sometimes I feel like I dont say this enough, but I really appreciate you. \\nGirl:  And I appreciate your words they lift me up. \\nBoy:  I saw you helping out at the shelter youre truly inspiring. \\nGirl:  Its just something small, but Im glad you noticed. \\nBoy:  How are you really feeling about everything? \\nGirl:  Its hard, but having you here helps more than you know. \\nBoy:  If you ever need to talk or just escape for a bit, Im here. \\nGirl:  Thank you, thats very comforting. \\nBoy:  I miss the way things used to be between us. \\nGirl:  Me too. Maybe we can find our way back.'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  Me too. Maybe we can find our way back. \\nBoy:  Do you remember our first trip together? \\nGirl:  How could I forget? It was magical. \\nBoy:  Lets make plans to go somewhere again, just the two of us. \\nGirl:  I would love that, really. \\nBoy:  Sometimes I feel like Im not enough for you. \\nGirl:  You are more than enough, and I love you just as you are. \\nBoy:  It hurts me to see you struggle and not be able to fix it. \\nGirl:  Your support is more powerful than you know. \\nBoy:  I got us tickets to that concert you wanted to go to. \\nGirl:  Really? Thats amazing! Youre the best. \\nBoy:  Its been tough at work, but coming home to you makes it all better. \\nGirl:  You make home a beautiful place to be. \\nBoy:  I worry about the future sometimes. \\nGirl:  Lets face it together, no matter what. \\nBoy:  I feel like Im always messing things up. \\nGirl:  We all make mistakes, but you always try to make things right, and thats what matters. \\nBoy:  Whats been the happiest moment for you recently?'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  This moment, right here with you. \\nBoy:  I know I dont always get it right, but I hope you know how much I love you. \\nGirl:  I do, and I love you all the more for your honesty. \\nBoy:  Sometimes I think about moving to a new city. Would you come with me? \\nGirl:  Anywhere, as long as were together. \\nBoy:  When you smile, it feels like everything is going to be okay. \\nGirl:  Your smile has the same effect on me. \\nBoy:  Im sorry for the things I said last night. I was out of line. \\nGirl:  I forgive you. Lets work on understanding each other better. \\nBoy:  Can we just drop everything and go for a drive? I need to clear my head. \\nGirl:  Lets go. I could use a little wind in my hair too. \\nBoy:  Youve been quiet today, whats on your mind? \\nGirl:  Just thinking about how lucky I am to have you. \\nBoy:  Im planning something special for your birthday. I hope youll like it. \\nGirl:  Knowing you, Im sure I will. \\nBoy:  I feel like weve been drifting apart. Can we talk about it?'),\n Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Girl:  Yes, we definitely should. I want to be close again. \\nBoy:  Theres this fear I cant shake that one day youll leave. \\nGirl:  Im not going anywhere. Im here for the long haul. \\nBoy:  Sometimes I wonder how someone as amazing as you chose me. \\nGirl:  I wonder the same about you choosing me. \\nBoy:  Youre my rock. Without you, I dont know where Id be. \\nGirl:  And youre my safe haven. Together, were unstoppable. \\nBoy:  Do you think were soulmates? \\nGirl:  I dont believe in soulmates, but I do believe in us. \\nBoy:  Every day with you is a new adventure. \\nGirl:  Thats because you make even the ordinary feel extraordinary. \\nBoy:  When Im with you, it feels like time stands still. \\nGirl:  Thats how you know its real. Time flies and stands still all at once.')]"},"metadata":{}}],"execution_count":62},{"cell_type":"code","source":"docs[3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:24.173328Z","iopub.execute_input":"2025-04-11T11:41:24.173628Z","iopub.status.idle":"2025-04-11T11:41:24.190146Z","shell.execute_reply.started":"2025-04-11T11:41:24.173602Z","shell.execute_reply":"2025-04-11T11:41:24.189006Z"}},"outputs":[{"execution_count":63,"output_type":"execute_result","data":{"text/plain":"Document(metadata={'source': '/kaggle/working/cleaned_text.txt'}, page_content='Boy:  I always saw you like no one ever had. \\nGirl:  Thats true. The way you see into my heart and soul, the way you know my emotions. Only you have that power over me. \\nBoy:  Really? \\nGirl:  I wouldnt lie to you about this. I never had done it before. \\nBoy:  I know you wouldnt, you are sacred for me. You mean the world to me. \\nGirl:  You mean everything to me, and if we get our chance I hope it can last forever. \\nBoy:  I promise it will. I wont be the same... Im going to do everything for you, whatever it takes to make you happy, thats what my heart tells me to do. \\nGirl:  I. \\nBoy:  You dont have to say something its okay. \\nGirl:  Thank you. \\nBoy:  Dont say thank you, I mean it. This is from my heart. I meant every single word I say to you. \\nGirl:  This feels like in the movies. \\nBoy:  So true so real. Is this pure? \\nGirl:  This is more than pure. \\nBoy:  Nothing is going to get between us. \\nGirl:  How do you know? \\nBoy:  I can see it in your eyes I can feel it in my heart.')"},"metadata":{}}],"execution_count":63},{"cell_type":"code","source":"docs[3].page_content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:24.191300Z","iopub.execute_input":"2025-04-11T11:41:24.191642Z","iopub.status.idle":"2025-04-11T11:41:24.207328Z","shell.execute_reply.started":"2025-04-11T11:41:24.191614Z","shell.execute_reply":"2025-04-11T11:41:24.206445Z"}},"outputs":[{"execution_count":64,"output_type":"execute_result","data":{"text/plain":"'Boy:  I always saw you like no one ever had. \\nGirl:  Thats true. The way you see into my heart and soul, the way you know my emotions. Only you have that power over me. \\nBoy:  Really? \\nGirl:  I wouldnt lie to you about this. I never had done it before. \\nBoy:  I know you wouldnt, you are sacred for me. You mean the world to me. \\nGirl:  You mean everything to me, and if we get our chance I hope it can last forever. \\nBoy:  I promise it will. I wont be the same... Im going to do everything for you, whatever it takes to make you happy, thats what my heart tells me to do. \\nGirl:  I. \\nBoy:  You dont have to say something its okay. \\nGirl:  Thank you. \\nBoy:  Dont say thank you, I mean it. This is from my heart. I meant every single word I say to you. \\nGirl:  This feels like in the movies. \\nBoy:  So true so real. Is this pure? \\nGirl:  This is more than pure. \\nBoy:  Nothing is going to get between us. \\nGirl:  How do you know? \\nBoy:  I can see it in your eyes I can feel it in my heart.'"},"metadata":{}}],"execution_count":64},{"cell_type":"code","source":"from langchain.embeddings import SentenceTransformerEmbeddings\nembeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:24.208340Z","iopub.execute_input":"2025-04-11T11:41:24.208715Z","iopub.status.idle":"2025-04-11T11:41:25.183348Z","shell.execute_reply.started":"2025-04-11T11:41:24.208685Z","shell.execute_reply":"2025-04-11T11:41:25.182053Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# !pip install pinecone-client -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:44:12.225177Z","iopub.execute_input":"2025-04-11T11:44:12.225673Z","iopub.status.idle":"2025-04-11T11:44:12.230537Z","shell.execute_reply.started":"2025-04-11T11:44:12.225637Z","shell.execute_reply":"2025-04-11T11:44:12.229126Z"}},"outputs":[],"execution_count":106},{"cell_type":"code","source":"# pip uninstall pinecone-client -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:29.519357Z","iopub.execute_input":"2025-04-11T11:41:29.519687Z","iopub.status.idle":"2025-04-11T11:41:29.524402Z","shell.execute_reply.started":"2025-04-11T11:41:29.519657Z","shell.execute_reply":"2025-04-11T11:41:29.523345Z"}},"outputs":[],"execution_count":67},{"cell_type":"code","source":"pip install --upgrade pinecone","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:29.525529Z","iopub.execute_input":"2025-04-11T11:41:29.525836Z","iopub.status.idle":"2025-04-11T11:41:34.232092Z","shell.execute_reply.started":"2025-04-11T11:41:29.525803Z","shell.execute_reply":"2025-04-11T11:41:34.230789Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (5.1.0)\nCollecting pinecone\n  Using cached pinecone-6.0.2-py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2025.1.31)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\nUsing cached pinecone-6.0.2-py3-none-any.whl (421 kB)\nInstalling collected packages: pinecone\n  Attempting uninstall: pinecone\n    Found existing installation: pinecone 5.1.0\n    Uninstalling pinecone-5.1.0:\n      Successfully uninstalled pinecone-5.1.0\nSuccessfully installed pinecone-6.0.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":68},{"cell_type":"markdown","source":"create an instance of the Pinecone","metadata":{}},{"cell_type":"code","source":"pip install \"pinecone[grpc]\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:34.233408Z","iopub.execute_input":"2025-04-11T11:41:34.233806Z","iopub.status.idle":"2025-04-11T11:41:39.280641Z","shell.execute_reply.started":"2025-04-11T11:41:34.233773Z","shell.execute_reply":"2025-04-11T11:41:39.278982Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pinecone[grpc] in /usr/local/lib/python3.10/dist-packages (6.0.2)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (2025.1.31)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (0.0.7)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (2.3.0)\nRequirement already satisfied: googleapis-common-protos>=1.66.0 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (1.66.0)\nRequirement already satisfied: grpcio>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (1.68.1)\nRequirement already satisfied: lz4>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (4.4.4)\nCollecting protobuf<6.0,>=5.29 (from pinecone[grpc])\n  Using cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\nRequirement already satisfied: protoc-gen-openapiv2<0.0.2,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (0.0.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone[grpc]) (1.17.0)\nUsing cached protobuf-5.29.4-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\nInstalling collected packages: protobuf\n  Attempting uninstall: protobuf\n    Found existing installation: protobuf 4.25.6\n    Uninstalling protobuf-4.25.6:\n      Successfully uninstalled protobuf-4.25.6\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngoogle-cloud-automl 1.0.1 requires google-api-core[grpc]<2.0.0dev,>=1.14.0, but you have google-api-core 2.24.2 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.29.4 which is incompatible.\ntensorflow 2.17.1 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\ntensorflow-decision-forests 1.10.0 requires tensorflow==2.17.0, but you have tensorflow 2.17.1 which is incompatible.\ntensorflow-metadata 1.13.1 requires protobuf<5,>=3.20.3, but you have protobuf 5.29.4 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed protobuf-5.29.4\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":69},{"cell_type":"code","source":"pip show pinecone","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:39.282234Z","iopub.execute_input":"2025-04-11T11:41:39.282649Z","iopub.status.idle":"2025-04-11T11:41:42.763075Z","shell.execute_reply.started":"2025-04-11T11:41:39.282601Z","shell.execute_reply":"2025-04-11T11:41:42.761816Z"}},"outputs":[{"name":"stdout","text":"Name: pinecone\nVersion: 6.0.2\nSummary: Pinecone client and SDK\nHome-page: https://www.pinecone.io\nAuthor: Pinecone Systems, Inc.\nAuthor-email: support@pinecone.io\nLicense: Apache-2.0\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: certifi, pinecone-plugin-interface, python-dateutil, typing-extensions, urllib3\nRequired-by: \nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":70},{"cell_type":"code","source":"# For HTTP:\n!pip install pinecone --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:42.764412Z","iopub.execute_input":"2025-04-11T11:41:42.764791Z","iopub.status.idle":"2025-04-11T11:41:47.014751Z","shell.execute_reply.started":"2025-04-11T11:41:42.764756Z","shell.execute_reply":"2025-04-11T11:41:47.013230Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (6.0.2)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2025.1.31)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n","output_type":"stream"}],"execution_count":71},{"cell_type":"code","source":"pip install \"pinecone[grpc]\" --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:47.015982Z","iopub.execute_input":"2025-04-11T11:41:47.016310Z","iopub.status.idle":"2025-04-11T11:41:51.303682Z","shell.execute_reply.started":"2025-04-11T11:41:47.016280Z","shell.execute_reply":"2025-04-11T11:41:51.302366Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pinecone[grpc] in /usr/local/lib/python3.10/dist-packages (6.0.2)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (2025.1.31)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (0.0.7)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (2.3.0)\nRequirement already satisfied: googleapis-common-protos>=1.66.0 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (1.66.0)\nRequirement already satisfied: grpcio>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (1.68.1)\nRequirement already satisfied: lz4>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (4.4.4)\nRequirement already satisfied: protobuf<6.0,>=5.29 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (5.29.4)\nRequirement already satisfied: protoc-gen-openapiv2<0.0.2,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (0.0.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone[grpc]) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":72},{"cell_type":"code","source":"# !pip install -qU \\\n#   \"pinecone[grpc]\"==5.1.0 \\\n#   pinecone-datasets==0.7.0 \\\n#   langchain-pinecone==0.1.2 \\\n#   langchain-openai==0.1.23 \\\n#   langchain==0.2.15","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:51.309052Z","iopub.execute_input":"2025-04-11T11:41:51.309364Z","iopub.status.idle":"2025-04-11T11:41:51.313422Z","shell.execute_reply.started":"2025-04-11T11:41:51.309335Z","shell.execute_reply":"2025-04-11T11:41:51.312516Z"}},"outputs":[],"execution_count":73},{"cell_type":"code","source":"!pip install pinecone","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:51.315354Z","iopub.execute_input":"2025-04-11T11:41:51.315674Z","iopub.status.idle":"2025-04-11T11:41:57.003958Z","shell.execute_reply.started":"2025-04-11T11:41:51.315650Z","shell.execute_reply":"2025-04-11T11:41:57.002676Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (6.0.2)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2025.1.31)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\n","output_type":"stream"}],"execution_count":74},{"cell_type":"code","source":"# from pinecone.grpc import PineconeGRPC as Pinecone\n# from pinecone import ServerlessSpec\n\n# pc = Pinecone(api_key=\"pcsk_vHf69_7u1ntwcfZtKaJvv6fyiMa4LEERHthZ7doGtT2qJei8TAw3acJysg3do6sjdz34Z\")\n# #pcsk_vHf69_7u1ntwcfZtKaJvv6fyiMa4LEERHthZ7doGtT2qJei8TAw3acJysg3do6sjdz34Z\n# index_name = \"rchatbot\"\n# if not pc.has_index(index_name):\n#     pc.create_index(\n#         name=index_name,\n#         dimension=384,\n#         metric=\"cosine\",\n#         spec=ServerlessSpec(\n#             cloud=\"aws\",\n#             region=\"us-east-1\"\n#         )\n#     )\n#     print(f\"Index '{index_name}' created successfully.\")\n# else:\n#     print(f\"Index '{index_name}' already exists.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:57.005426Z","iopub.execute_input":"2025-04-11T11:41:57.005853Z","iopub.status.idle":"2025-04-11T11:41:57.010470Z","shell.execute_reply.started":"2025-04-11T11:41:57.005814Z","shell.execute_reply":"2025-04-11T11:41:57.009493Z"}},"outputs":[],"execution_count":75},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"index_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:57.011649Z","iopub.execute_input":"2025-04-11T11:41:57.012079Z","iopub.status.idle":"2025-04-11T11:41:57.077666Z","shell.execute_reply.started":"2025-04-11T11:41:57.012042Z","shell.execute_reply":"2025-04-11T11:41:57.076634Z"}},"outputs":[{"execution_count":76,"output_type":"execute_result","data":{"text/plain":"'cstagecover'"},"metadata":{}}],"execution_count":76},{"cell_type":"markdown","source":"if INDEX is created then you need to interact with it and perform queries or upsert vectors you dont need to create it again if it exist","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from pinecone.grpc import PineconeGRPC as Pinecone\n\n# pc.describe_index(\"rchatbot\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:57.078695Z","iopub.execute_input":"2025-04-11T11:41:57.079018Z","iopub.status.idle":"2025-04-11T11:41:57.092430Z","shell.execute_reply.started":"2025-04-11T11:41:57.078982Z","shell.execute_reply":"2025-04-11T11:41:57.091279Z"}},"outputs":[],"execution_count":77},{"cell_type":"markdown","source":"describe index tell details information about pinecone index. pc means instance of pinecone client  connect this to pinecone service using API . PC is used to create describe delete and interact with indexes","metadata":{}},{"cell_type":"markdown","source":"Why Upsert is Needed:\n\n    Upsert: You need to upsert the embeddings into Pinecone because, once the embeddings are in the index, they can be searched for similarity against a query. You only need to perform the upsert once for each document and store the embeddings in the index.\n\n    Querying: After upserting the documents (embeddings), you can query the index at any time with a new query (like \"Are those moments coming back?\") and retrieve the most similar documents.","metadata":{}},{"cell_type":"markdown","source":"Document Loading:\n\n    The document is loaded using the TextLoader from Langchain.\n\n    RecursiveCharacterTextSplitter is used to split large documents into smaller chunks based on the chunk_size (1000 characters) and chunk_overlap (50 characters).\n\nEmbeddings Generation:\n\n    For each chunk of text, we generate an embedding. In this example, the generate_embeddings function is a placeholder that returns a mock embedding (replace this with the actual embedding logic from your model like llama-text-embed-v2, Sentence-BERT, or another model).\n\nUpsert into Pinecone:\n\n    The records list is created, containing the document ID, its embedding vector (values), and any additional metadata (such as the document content).\n\n    The index.upsert(vectors=records) call inserts the vectors into the Pinecone index.\n\nQuerying Pinecone:\n\n    A query (\"Are those moments coming back?\") is converted to an embedding vector using the generate_embeddings function.\n\n    The query method in Pinecone is used to find the most similar vectors to the query vector. The top_k=3 parameter specifies that we want to retrieve the top 3 most similar results.\n\n    The results include metadata, so we print the id, score, and metadata for each hit.","metadata":{}},{"cell_type":"code","source":"# pip install --upgrade pinecone-client","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:57.093652Z","iopub.execute_input":"2025-04-11T11:41:57.094036Z","iopub.status.idle":"2025-04-11T11:41:57.111426Z","shell.execute_reply.started":"2025-04-11T11:41:57.093998Z","shell.execute_reply":"2025-04-11T11:41:57.110331Z"}},"outputs":[],"execution_count":78},{"cell_type":"markdown","source":"Solution:\n\nTo resolve this, you need to ensure that:\n\n    The vectors you generate (e.g., using generate_embeddings()) must have 1024 dimensions, matching the dimension of the Pinecone index.\n\n    The dimension of the index and the vectors must match to allow successful upsert and queries.","metadata":{}},{"cell_type":"markdown","source":"Pinecone's client API has changed. In newer versions of the Pinecone client, the init method has been removed, and you must instead create an instance of the Pinecone class to interact with Pinecone's API.\n","metadata":{}},{"cell_type":"code","source":"pip uninstall -y pinecone-client","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:57.112521Z","iopub.execute_input":"2025-04-11T11:41:57.112823Z","iopub.status.idle":"2025-04-11T11:41:58.110658Z","shell.execute_reply.started":"2025-04-11T11:41:57.112798Z","shell.execute_reply":"2025-04-11T11:41:58.109353Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: pinecone-client 6.0.0\nUninstalling pinecone-client-6.0.0:\n  Successfully uninstalled pinecone-client-6.0.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":79},{"cell_type":"code","source":"pip install pinecone","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:41:58.112169Z","iopub.execute_input":"2025-04-11T11:41:58.112610Z","iopub.status.idle":"2025-04-11T11:42:02.209120Z","shell.execute_reply.started":"2025-04-11T11:41:58.112566Z","shell.execute_reply":"2025-04-11T11:42:02.207714Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pinecone in /usr/local/lib/python3.10/dist-packages (6.0.2)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2025.1.31)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone) (0.0.7)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone) (2.3.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":80},{"cell_type":"code","source":"pip install \"pinecone[grpc]\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:02.210598Z","iopub.execute_input":"2025-04-11T11:42:02.210994Z","iopub.status.idle":"2025-04-11T11:42:06.330923Z","shell.execute_reply.started":"2025-04-11T11:42:02.210953Z","shell.execute_reply":"2025-04-11T11:42:06.329631Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pinecone[grpc] in /usr/local/lib/python3.10/dist-packages (6.0.2)\nRequirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (2025.1.31)\nRequirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (0.0.7)\nRequirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (2.9.0.post0)\nRequirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (4.12.2)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (2.3.0)\nRequirement already satisfied: googleapis-common-protos>=1.66.0 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (1.66.0)\nRequirement already satisfied: grpcio>=1.44.0 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (1.68.1)\nRequirement already satisfied: lz4>=3.1.3 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (4.4.4)\nRequirement already satisfied: protobuf<6.0,>=5.29 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (5.29.4)\nRequirement already satisfied: protoc-gen-openapiv2<0.0.2,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from pinecone[grpc]) (0.0.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.5.3->pinecone[grpc]) (1.17.0)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":81},{"cell_type":"markdown","source":"    1.Extract the content from your documents and use the Gemini API to generate embeddings for each of them.\n\n    2.Upsert the embeddings into Pinecone for storage and search.\n\n   3. Query Pinecone using a text query and retrieve the most similar document based on embeddings.\n\nSteps:\n\n   1. Process each document: You'll process the text stored in the documents you have (in this case, dialogue text).\n\n   2. Generate embeddings: Use the Gemini API to get embeddings for each document.\n\n   3. Upsert into Pinecone: Store the embeddings in Pinecone for similarity search.\n\n    4 Perform search: Perform a search using a query and get the most relevant documents.","metadata":{}},{"cell_type":"code","source":"# !pip install -qU \\\n#   \"pinecone[grpc]\"==5.1.0 \\\n#   pinecone-datasets==0.7.0 \\\n#   langchain-pinecone==0.1.2 \\\n#   langchain-openai==0.1.23 \\\n#   langchain==0.2.15","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:06.332080Z","iopub.execute_input":"2025-04-11T11:42:06.332448Z","iopub.status.idle":"2025-04-11T11:42:06.336573Z","shell.execute_reply.started":"2025-04-11T11:42:06.332416Z","shell.execute_reply":"2025-04-11T11:42:06.335596Z"}},"outputs":[],"execution_count":82},{"cell_type":"code","source":"pip show pinecone","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:06.337710Z","iopub.execute_input":"2025-04-11T11:42:06.338105Z","iopub.status.idle":"2025-04-11T11:42:11.518005Z","shell.execute_reply.started":"2025-04-11T11:42:06.338069Z","shell.execute_reply":"2025-04-11T11:42:11.516858Z"}},"outputs":[{"name":"stdout","text":"Name: pinecone\nVersion: 6.0.2\nSummary: Pinecone client and SDK\nHome-page: https://www.pinecone.io\nAuthor: Pinecone Systems, Inc.\nAuthor-email: support@pinecone.io\nLicense: Apache-2.0\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: certifi, pinecone-plugin-interface, python-dateutil, typing-extensions, urllib3\nRequired-by: \nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":83},{"cell_type":"code","source":"from langchain_pinecone import PineconeVectorStore","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:11.519261Z","iopub.execute_input":"2025-04-11T11:42:11.519627Z","iopub.status.idle":"2025-04-11T11:42:11.524345Z","shell.execute_reply.started":"2025-04-11T11:42:11.519587Z","shell.execute_reply":"2025-04-11T11:42:11.523126Z"}},"outputs":[],"execution_count":84},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"meaning of followng line is the environment variable is PINECONE_API_KEY, this has not assign to python variable explicitly nor is it referenced directly using ","metadata":{}},{"cell_type":"code","source":"# import os\n# os.environ['PINECONE_API_KEY'] = \"pcsk_vHf69_7u1ntwcfZtKaJvv6fyiMa4LEERHthZ7doGtT2qJei8TAw3acJysg3do6sjdz34Z\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:11.525437Z","iopub.execute_input":"2025-04-11T11:42:11.525747Z","iopub.status.idle":"2025-04-11T11:42:11.545398Z","shell.execute_reply.started":"2025-04-11T11:42:11.525724Z","shell.execute_reply":"2025-04-11T11:42:11.544483Z"}},"outputs":[],"execution_count":85},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom pinecone import Pinecone, ServerlessSpec\nfrom sentence_transformers import SentenceTransformer\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nimport torch\n\n\n# Initialize the embedding model\nembeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\nvectorstore = PineconeVectorStore.from_documents(\n    docs,\n    embeddings,\n    index_name=index_name\n)\nfrom langchain_pinecone import PineconeVectorStore\n\n# Perform the similarity search\nquery = \"Boy:  Because of what you feel for me. Does it mean something? \"\nresults = vectorstore.similarity_search(\n    query,  # your search query\n    k=1   # return 3 most relevant docs\n)\n\n# Print results\nfor doc in results:\n    print(f\"Content: {doc.page_content}\")\n    print(f\"Metadata: {doc.metadata}\")\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:11.546567Z","iopub.execute_input":"2025-04-11T11:42:11.546856Z","iopub.status.idle":"2025-04-11T11:42:15.167127Z","shell.execute_reply.started":"2025-04-11T11:42:11.546830Z","shell.execute_reply":"2025-04-11T11:42:15.166180Z"}},"outputs":[{"name":"stdout","text":"Content: Girl:  I just wanted to know what you felt. \nBoy:  (silence) \nGirl:  And for that, I had to play with your heart. \nBoy:  Why? \nGirl:  Eventually, you would hurt me if I didnt. I really dont want to be hurt anymore. \nBoy:  This is the worst thing you can do to me, to us. \nGirl:  Why do you believe theres still an us? \nBoy:  Because of what you feel for me. Does it mean something? \nGirl:  Maybe a lie. \nBoy:  Wow. \nGirl:  I love you. \nBoy:  Please dont say that. \nGirl:  Its true, I do. \nBoy:  So now you know. \nGirl:  What? \nBoy:  How I feel. \nGirl:  So, what does it feel to be vulnerable? \nBoy:  I feel discovered, unfolded. \nGirl:  Im sorry. You know what you made me feel. \nBoy:  I think. \nGirl:  So? \nBoy:  So? \nGirl:  I have to leave you now. \nBoy:  I still dont get it. \nGirl:  What, my prince? \nBoy:  Why do you want to hurt me? \nGirl:  Mm. \nBoy:  I know I dont deserve this. \nGirl:  I know you dont. \nBoy:  So why?\nMetadata: {'source': '/kaggle/working/cleaned_text.txt'}\n---\n","output_type":"stream"}],"execution_count":86},{"cell_type":"code","source":"pip show pinecone","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:15.168158Z","iopub.execute_input":"2025-04-11T11:42:15.168428Z","iopub.status.idle":"2025-04-11T11:42:18.580115Z","shell.execute_reply.started":"2025-04-11T11:42:15.168406Z","shell.execute_reply":"2025-04-11T11:42:18.578962Z"}},"outputs":[{"name":"stdout","text":"Name: pinecone\nVersion: 6.0.2\nSummary: Pinecone client and SDK\nHome-page: https://www.pinecone.io\nAuthor: Pinecone Systems, Inc.\nAuthor-email: support@pinecone.io\nLicense: Apache-2.0\nLocation: /usr/local/lib/python3.10/dist-packages\nRequires: certifi, pinecone-plugin-interface, python-dateutil, typing-extensions, urllib3\nRequired-by: \nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":87},{"cell_type":"code","source":"# Get embeddings for the query\nquery_embedding = embeddings.embed_query(query)\nprint(f\"Query embedding dimension: {len(query_embedding)}\")\nprint(f\"Query embedding vector: {query_embedding[:5]}...\")\n\n# Get embeddings for each document\nfor doc in results:\n    doc_embedding = embeddings.embed_query(doc.page_content)\n    print(f\"\\nDocument content: {doc.page_content[:100]}...\")\n    print(f\"Document embedding dimension: {len(doc_embedding)}\")\n    print(f\"Document embedding vector: {doc_embedding[:5]}...\")\n    print(f\"Metadata: {doc.metadata}\")\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:18.581263Z","iopub.execute_input":"2025-04-11T11:42:18.581631Z","iopub.status.idle":"2025-04-11T11:42:18.683921Z","shell.execute_reply.started":"2025-04-11T11:42:18.581601Z","shell.execute_reply":"2025-04-11T11:42:18.683021Z"}},"outputs":[{"name":"stdout","text":"Query embedding dimension: 384\nQuery embedding vector: [-0.07622136920690536, 0.07195234298706055, 0.10226672142744064, -0.045930493623018265, 0.11633729189634323]...\n\nDocument content: Girl:  I just wanted to know what you felt. \nBoy:  (silence) \nGirl:  And for that, I had to play wit...\nDocument embedding dimension: 384\nDocument embedding vector: [-0.09600366652011871, 0.025125723332166672, 0.10755997896194458, 0.038372065871953964, 0.07125211507081985]...\nMetadata: {'source': '/kaggle/working/cleaned_text.txt'}\n---\n","output_type":"stream"}],"execution_count":88},{"cell_type":"markdown","source":"index_name = Just the name of your Pinecone index (string).\n\nindex = An object that interacts with Pinecone (stores and retrieves vectors).","metadata":{}},{"cell_type":"code","source":"index_name","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:18.684937Z","iopub.execute_input":"2025-04-11T11:42:18.685327Z","iopub.status.idle":"2025-04-11T11:42:18.691595Z","shell.execute_reply.started":"2025-04-11T11:42:18.685291Z","shell.execute_reply":"2025-04-11T11:42:18.690540Z"}},"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"'cstagecover'"},"metadata":{}}],"execution_count":89},{"cell_type":"markdown","source":"find the describe only for object not for name of your pinecone index as string. pinecone index object store the data and answer the query data name cant aswer or store anythings","metadata":{}},{"cell_type":"code","source":"index = pc.Index(\"rchatbot\")  # Replace with your index name\n\n# Get index statistics\nstats = index.describe_index_stats()\nprint(stats) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:18.692422Z","iopub.execute_input":"2025-04-11T11:42:18.692762Z","iopub.status.idle":"2025-04-11T11:42:19.070035Z","shell.execute_reply.started":"2025-04-11T11:42:18.692736Z","shell.execute_reply":"2025-04-11T11:42:19.068882Z"}},"outputs":[{"name":"stdout","text":"{'dimension': 384,\n 'index_fullness': 0.0,\n 'namespaces': {'': {'vector_count': 344},\n                'relationship_chat': {'vector_count': 66}},\n 'total_vector_count': 410}\n","output_type":"stream"}],"execution_count":90},{"cell_type":"code","source":"# import os\n\n# print(\"PINECONE_API_KEY:\", os.environ.get(\"PINECONE_API_KEY\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:19.070996Z","iopub.execute_input":"2025-04-11T11:42:19.071295Z","iopub.status.idle":"2025-04-11T11:42:19.075283Z","shell.execute_reply.started":"2025-04-11T11:42:19.071269Z","shell.execute_reply":"2025-04-11T11:42:19.074204Z"}},"outputs":[],"execution_count":91},{"cell_type":"code","source":"# import pinecone\n\n# # Initialize Pinecone\n# pc = pinecone.Pinecone(api_key=\"pcsk_vHf69_7u1ntwcfZtKaJvv6fyiMa4LEERHthZ7doGtT2qJei8TAw3acJysg3do6sjdz34Z\")\n\n# # List available indexes\n# print(\"Available indexes:\", pc.list_indexes())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:19.076203Z","iopub.execute_input":"2025-04-11T11:42:19.076494Z","iopub.status.idle":"2025-04-11T11:42:19.093184Z","shell.execute_reply.started":"2025-04-11T11:42:19.076444Z","shell.execute_reply":"2025-04-11T11:42:19.092142Z"}},"outputs":[],"execution_count":92},{"cell_type":"code","source":"# # Describe the index to get all namespaces and vector counts\n# stats = pc.Index(\"rchatbot\").describe_index_stats()\n\n# # Print the namespaces (these contain vector counts)\n# print(stats[\"namespaces\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:19.094217Z","iopub.execute_input":"2025-04-11T11:42:19.094552Z","iopub.status.idle":"2025-04-11T11:42:19.111693Z","shell.execute_reply.started":"2025-04-11T11:42:19.094516Z","shell.execute_reply":"2025-04-11T11:42:19.110704Z"}},"outputs":[],"execution_count":93},{"cell_type":"code","source":"# # Assuming `index` is already initialized\n# vector_id = \"your_vector_id_here\"  # Replace with an actual vector ID\n\n# vector_data = index.fetch([vector_id])  # Fetch vector from Pinecone\n# print(vector_data)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:19.112750Z","iopub.execute_input":"2025-04-11T11:42:19.113035Z","iopub.status.idle":"2025-04-11T11:42:19.131164Z","shell.execute_reply.started":"2025-04-11T11:42:19.113010Z","shell.execute_reply":"2025-04-11T11:42:19.130199Z"}},"outputs":[],"execution_count":94},{"cell_type":"code","source":"# Search with scores\nquery = \"It has, but why?\"\nresults = vectorstore.similarity_search_with_score(\n    query,\n    k=3\n)\n\n# Print results with scores\nfor doc, score in results:\n    print(f\"Score: {score}\")\n    print(f\"Content: {doc.page_content}\")\n    print(f\"Metadata: {doc.metadata}\")\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:19.132162Z","iopub.execute_input":"2025-04-11T11:42:19.132442Z","iopub.status.idle":"2025-04-11T11:42:19.232473Z","shell.execute_reply.started":"2025-04-11T11:42:19.132411Z","shell.execute_reply":"2025-04-11T11:42:19.231385Z"}},"outputs":[{"name":"stdout","text":"Score: 0.0620855801\nContent: Girl:  I just wanted to know what you felt. \nBoy:  (silence) \nGirl:  And for that, I had to play with your heart. \nBoy:  Why? \nGirl:  Eventually, you would hurt me if I didnt. I really dont want to be hurt anymore. \nBoy:  This is the worst thing you can do to me, to us. \nGirl:  Why do you believe theres still an us? \nBoy:  Because of what you feel for me. Does it mean something? \nGirl:  Maybe a lie. \nBoy:  Wow. \nGirl:  I love you. \nBoy:  Please dont say that. \nGirl:  Its true, I do. \nBoy:  So now you know. \nGirl:  What? \nBoy:  How I feel. \nGirl:  So, what does it feel to be vulnerable? \nBoy:  I feel discovered, unfolded. \nGirl:  Im sorry. You know what you made me feel. \nBoy:  I think. \nGirl:  So? \nBoy:  So? \nGirl:  I have to leave you now. \nBoy:  I still dont get it. \nGirl:  What, my prince? \nBoy:  Why do you want to hurt me? \nGirl:  Mm. \nBoy:  I know I dont deserve this. \nGirl:  I know you dont. \nBoy:  So why?\nMetadata: {'source': '/kaggle/working/cleaned_text.txt'}\n---\nScore: 0.0620855801\nContent: Girl:  I just wanted to know what you felt. \nBoy:  (silence) \nGirl:  And for that, I had to play with your heart. \nBoy:  Why? \nGirl:  Eventually, you would hurt me if I didnt. I really dont want to be hurt anymore. \nBoy:  This is the worst thing you can do to me, to us. \nGirl:  Why do you believe theres still an us? \nBoy:  Because of what you feel for me. Does it mean something? \nGirl:  Maybe a lie. \nBoy:  Wow. \nGirl:  I love you. \nBoy:  Please dont say that. \nGirl:  Its true, I do. \nBoy:  So now you know. \nGirl:  What? \nBoy:  How I feel. \nGirl:  So, what does it feel to be vulnerable? \nBoy:  I feel discovered, unfolded. \nGirl:  Im sorry. You know what you made me feel. \nBoy:  I think. \nGirl:  So? \nBoy:  So? \nGirl:  I have to leave you now. \nBoy:  I still dont get it. \nGirl:  What, my prince? \nBoy:  Why do you want to hurt me? \nGirl:  Mm. \nBoy:  I know I dont deserve this. \nGirl:  I know you dont. \nBoy:  So why?\nMetadata: {'source': '/kaggle/working/cleaned_text.txt'}\n---\nScore: 0.0620027669\nContent: Girl:  I just wanted to know what you felt. \nBoy:  (silence) \nGirl:  And for that, I had to play with your heart. \nBoy:  Why? \nGirl:  Eventually, you would hurt me if I didnt. I really dont want to be hurt anymore. \nBoy:  This is the worst thing you can do to me, to us. \nGirl:  Why do you believe theres still an us? \nBoy:  Because of what you feel for me. Does it mean something? \nGirl:  Maybe a lie. \nBoy:  Wow. \nGirl:  I love you. \nBoy:  Please dont say that. \nGirl:  Its true, I do. \nBoy:  So now you know. \nGirl:  What? \nBoy:  How I feel. \nGirl:  So, what does it feel to be vulnerable? \nBoy:  I feel discovered, unfolded. \nGirl:  Im sorry. You know what you made me feel. \nBoy:  I think. \nGirl:  So? \nBoy:  So? \nGirl:  I have to leave you now. \nBoy:  I still dont get it. \nGirl:  What, my prince? \nBoy:  Why do you want to hurt me? \nGirl:  Mm. \nBoy:  I know I dont deserve this. \nGirl:  I know you dont. \nBoy:  So why?\nMetadata: {'source': '/kaggle/working/cleaned_text.txt'}\n---\n","output_type":"stream"}],"execution_count":95},{"cell_type":"code","source":"# Search with scores\nquery = \"Girl:  Eventually, you would hurt me if I didnt. I really dont want to be hurt anymore.?\"\nresults = vectorstore.similarity_search_with_score(\n    query,\n    k=1\n)\n# Print results with scores\nfor doc, score in results:\n    print(f\"Score: {score}\")\n    print(f\"Content: {doc.page_content}\")\n    print(f\"Metadata: {doc.metadata}\")\n    print(\"---\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:19.233572Z","iopub.execute_input":"2025-04-11T11:42:19.233867Z","iopub.status.idle":"2025-04-11T11:42:19.290326Z","shell.execute_reply.started":"2025-04-11T11:42:19.233843Z","shell.execute_reply":"2025-04-11T11:42:19.289379Z"}},"outputs":[{"name":"stdout","text":"Score: 0.455652058\nContent: Girl:  . \nBoy:  Sweetie! Talk to me!! \nGirl:  I. \nBoy:  You what?? YOU WHAT??? \nGirl:  I have cancer and Im on life support. \nBoy:  Breaks into tears. \nGirl:  Theyre taking me off tonight. \nBoy:  Why? \nGirl:  I wanted to tell you, but I couldnt. \nBoy:  Why didnt you tell me? \nGirl:  I didnt want to hurt you. \nBoy:  You could never hurt me. \nGirl:  I just wanted to see if you felt about me the same as I felt about you. \nBoy:  .? \nGirl:  I love you more than anything. I would give you the world in a heartbeat. I would die for you and take a bullet for you. \nBoy:  (crying) \nGirl:  Dont be sad. I love you and Ill always be here with you. \nBoy:  Then why did you break up with me? \nGirl:  Hi there! \nBoy:  Hi! Will you be my friend? \nGirl:  Only if you promise not to hurt me. \nBoy:  What? \nGirl:  My sister was crying last night and said to never be friends with a boy who will hurt you. So will you promise not to hurt me? \nBoy:  I could never hurt you. At a Wedding Seventeen Years Later\nMetadata: {'source': '/kaggle/working/cleaned_text.txt'}\n---\n","output_type":"stream"}],"execution_count":96},{"cell_type":"markdown","source":"Key benefits of keeping your current setup:\n\n    SentenceTransformers is designed for semantic similarity tasks 2\n    The model you're using (all-MiniLM-L6-v2) is optimized for this type of work\n    It integrates well with LangChain and Pinecone 1\n\nFor your relationship-oriented chat application, this embedding model is suitable because:\n\n    It handles semantic similarity well\n    It's already integrated with your vector database\n    It works with your existing similarity search implementation\n\nYou only need to add the Gemini API integration for generating responses while keeping your current embedding setup intact","metadata":{}},{"cell_type":"markdown","source":"Key Components to Build:\n\n    Sentiment Analysis Module:\n\n    Analyze conversation tone and emotional context\n    Track interest levels over time\n    Store sentiment scores in metadata\n\n    Sentence Improvement System:\n\n    Compare user input against similar conversations\n    Generate contextually appropriate suggestions\n    Consider relationship stage and conversation history\n\n    Relationship Stage Tracking:\n\n    Define relationship stages (e.g., initial, developing, established)\n    Track progression through conversation analysis\n    Adjust suggestions based on stage\n\n    Recommendation Engine:\n\n    Use conversation history to generate relevant responses\n    Consider sentiment and relationship stage\n    Provide multiple suggestion options\n\n    User Interface:\n\n    Real-time conversation interface\n    Display improvement suggestions\n    Show relationship insights\n\nNext Steps:\n\n    Implement sentiment analysis integration\n    Build sentence improvement logic\n    Create relationship stage tracking\n    Develop recommendation system\n    Design and implement UI\n    Test and refine the system\n\n\nNext steps to implement:\n\n    Build sentiment analysis:\n        Process conversation text\n        Calculate sentiment scores\n        Store results in Pinecone metadata\n\n    Create relationship context tracking:\n        Define relationship stages\n        Track conversation progression\n        Update vector store with new context\n\n    Implement suggestion system:\n        Query similar conversations\n        Generate contextual responses\n        Rank and filter suggestions\n\n    Add conversation summarization:\n        Periodically summarize conversations\n        Update long-term memory\n        Clean up old conversations\n\n\nThis provides a framework for building your relationship-oriented chat application while leveraging your existing vector database infrastructure.","metadata":{}},{"cell_type":"markdown","source":"fllowing implementation :---\n\n    Uses your existing embeddings\n    Maintains conversation history\n    Tracks sentiment and relationship stages\n    Stores everything in Pinecone\n    Provides contextual responses\n\n","metadata":{}},{"cell_type":"code","source":"import google.generativeai as genai\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport os\nYOUR_GEMINI_API_KEY = 'AIzaSyCKPn2qhjs4HGdWoYVuXnt8ssfDlFwGFbc'\n# Configure Gemini\ngenai.configure(api_key=\"YOUR_GEMINI_API_KEY\")\nmodel = genai.GenerativeModel('gemini-pro')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:19.291285Z","iopub.execute_input":"2025-04-11T11:42:19.291592Z","iopub.status.idle":"2025-04-11T11:42:19.296522Z","shell.execute_reply.started":"2025-04-11T11:42:19.291567Z","shell.execute_reply":"2025-04-11T11:42:19.295330Z"}},"outputs":[],"execution_count":97},{"cell_type":"markdown","source":"The system will:\n\n    Take input from the boy\n    Generate appropriate responses from the girl\n    Track conversation sentiment and stage\n    Store everything in Pinecone for context\n    Use previous conversations to inform responses\n","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"For the Pinecone parts of your code, make sure to:\n\nInitialize Pinecone properly with your API key\nCreate an index with the correct dimension size for your embeddings\nUse the proper format for upserting vectors with metadata\nEnsure your index is properly initialized before using it in the chat loop","metadata":{}},{"cell_type":"code","source":"import requests\nimport json\n\n# Function to call Gemini API with a user-defined question\ndef call_gemini_api(question):\n    # Set your Gemini API key here\n    api_key = 'AIzaSyCKPn2qhjs4HGdWoYVuXnt8ssfDlFwGFbc'\n\n    # API URL\n    url = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}\"\n\n    # The data payload for the request\n    data = {\n        \"contents\": [{\n            \"parts\": [{\"text\": question}]\n        }]\n    }\n\n    # Headers\n    headers = {\n        'Content-Type': 'application/json'\n    }\n\n    # Make the POST request to the API\n    response = requests.post(url, headers=headers, json=data)\n\n    # Check the response status and print the result\n    if response.status_code == 200:\n        response_data = response.json()\n        print(\"Response from Gemini API:\")\n        print(response_data)\n    else:\n        print(f\"Request failed with status code {response.status_code}\")\n        print(response.text)\n\n# Ask for a question to send to the Gemini API\nquestion = input(\"Enter your question for Gemini API: \")\n\n# Call the Gemini API with the user's question\ncall_gemini_api(question)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:19.297395Z","iopub.execute_input":"2025-04-11T11:42:19.297713Z","iopub.status.idle":"2025-04-11T11:42:35.138200Z","shell.execute_reply.started":"2025-04-11T11:42:19.297688Z","shell.execute_reply":"2025-04-11T11:42:35.137152Z"}},"outputs":[{"output_type":"stream","name":"stdin","text":"Enter your question for Gemini API:  hello\n"},{"name":"stdout","text":"Response from Gemini API:\n{'candidates': [{'content': {'parts': [{'text': 'Hello! How can I help you today?\\n'}], 'role': 'model'}, 'finishReason': 'STOP', 'avgLogprobs': -0.043426874279975894}], 'usageMetadata': {'promptTokenCount': 1, 'candidatesTokenCount': 10, 'totalTokenCount': 11, 'promptTokensDetails': [{'modality': 'TEXT', 'tokenCount': 1}], 'candidatesTokensDetails': [{'modality': 'TEXT', 'tokenCount': 10}]}, 'modelVersion': 'gemini-2.0-flash'}\n","output_type":"stream"}],"execution_count":98},{"cell_type":"markdown","source":"using api key set the enviornment","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"fllowing code for NEXT SENTENSE IMPROVEMNT SUGGESTION","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport pinecone\nimport requests\nimport json\nimport google.generativeai as genai\nfrom datetime import datetime\nfrom textblob import TextBlob\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom pinecone import Pinecone, ServerlessSpec\n\n# ============================\n# Environment & API Key Setup\n# ============================\nos.environ[\"PINECONE_API_KEY\"] = \"pcsk_vHf69_7u1ntwcfZtKaJvv6fyiMa4LEERHthZ7doGtT2qJei8TAw3acJysg3do6sjdz34Z\"\nos.environ[\"PINECONE_ENVIRONMENT\"] = \"us-east-1\"\nos.environ[\"PINECONE_INDEX_NAME\"] = \"rchatbot\"\n\nYOUR_GEMINI_API_KEY = 'AIzaSyCKPn2qhjs4HGdWoYVuXnt8ssfDlFwGFbc'\nPINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n\nNAMESPACE = \"relationship_chat\"\n\n# ============================\n# Initialize Pinecone and Index\n# ============================\npc = Pinecone(api_key=PINECONE_API_KEY)\nindex_name = os.environ[\"PINECONE_INDEX_NAME\"]\n\nif index_name not in pc.list_indexes().names():\n    print(f\"Index '{index_name}' not found, creating it...\")\n    pc.create_index(\n        name=index_name,\n        dimension=384,\n        metric='cosine',\n        spec=ServerlessSpec(cloud='aws', region=os.environ[\"PINECONE_ENVIRONMENT\"])\n    )\n    print(f\"Index '{index_name}' created successfully.\")\nelse:\n    print(f\"Index '{index_name}' already exists.\")\n\nindex = pc.Index(index_name)\n\n# ============================\n# Gemini API Setup (REST call)\n# ============================\nGEMINI_API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={YOUR_GEMINI_API_KEY}\"\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:35.139426Z","iopub.execute_input":"2025-04-11T11:42:35.139740Z","iopub.status.idle":"2025-04-11T11:42:35.386154Z","shell.execute_reply.started":"2025-04-11T11:42:35.139714Z","shell.execute_reply":"2025-04-11T11:42:35.384846Z"}},"outputs":[{"name":"stdout","text":"Index 'rchatbot' already exists.\n","output_type":"stream"}],"execution_count":99},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"integrating conversation memory buffer integrating instead of conversation buffer history","metadata":{}},{"cell_type":"markdown","source":"ConversationSummaryBufferMemory combines the two ideas. It keeps a buffer of recent interactions in memory, but rather than just completely flushing old interactions it compiles them into a summary and uses both.","metadata":{}},{"cell_type":"markdown","source":"Create logic through input and ouptu thinking and learned implemented logic","metadata":{}},{"cell_type":"markdown","source":"Based on the provided documentation of PINECONE, I cannot provide specific guidance on integrating ConversationSummaryMemory with the Gemini API, as the documentation only shows integration with Pinecone's own Assistant API which supports two models: gpt-4o and claude-3-5-sonnet \n\n\nMEANS NO MEMORY SUPPORT OR INTEGRATE WIHT GEMINI API KEY ","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Retrival from both conversation history and pinecone vecotorstore  ask chatbot of documentation trained is gemeni api support the conversation buffer memory or suggest any buffer memory","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport requests\nimport json\nfrom datetime import datetime\nfrom textblob import TextBlob  # For sentiment analysis\nimport pinecone\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# # # ============================\n# # # Initialize Environment and API Keys\n# # # ============================\n# os.environ[\"PINECONE_API_KEY\"] = \"pcsk_vHf69_7u1ntwcfZtKaJvv6fyiMa4LEERHthZ7doGtT2qJei8TAw3acJysg3do6sjdz34Z\"\n# os.environ[\"PINECONE_ENVIRONMENT\"] = \"us-east-1\"\n# os.environ[\"PINECONE_INDEX_NAME\"] = \"cstagecover\"\n# YOUR_GEMINI_API_KEY = 'AIzaSyCKPn2qhjs4HGdWoYVuXnt8ssfDlFwGFbc'\n# # PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n# # NAMESPACE = \"relationship_conver\"\n\n# ============================\n# Initialize Pinecone and Index\n# ============================\npc = pinecone.Pinecone(api_key=PINECONE_API_KEY)\nindex_name = os.environ[\"PINECONE_INDEX_NAME\"]\n\nif index_name not in pc.list_indexes().names():\n    print(f\"Index '{index_name}' not found, creating it...\")\n    pc.create_index(\n        name=index_name,\n        dimension=384,\n        metric='cosine',\n        spec=ServerlessSpec(cloud='aws', region=os.environ[\"PINECONE_ENVIRONMENT\"])\n    )\n    print(f\"Index '{index_name}' created successfully.\")\nelse:\n    print(f\"Index '{index_name}' already exists.\")\n\nindex = pc.Index(index_name)\n\n# ============================\n# Initialize the Embedding Model and Pinecone VectorStore\n# ============================\nembeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n# Load the documents and create PineconeVectorStore\n#docs = []  # Load your documents or past conversations here\nvectorstore = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)\n\n# ============================\n# Sentiment Analysis Function\n# ============================\ndef analyze_conversation(text):\n    analysis = TextBlob(text)\n    sentiment_score = analysis.sentiment.polarity\n    if sentiment_score > 0.5:\n        stage = \"positive/developing\"\n        emoji = \"😊\"  # Happy emoji for positive sentiment\n    elif sentiment_score > 0:\n        stage = \"neutral/initial\"\n        emoji = \"🙂\"  # Neutral emoji\n    else: \n        stage = \"negative/needs attention\"\n        emoji = \"😟\"  # Sad emoji for negative sentiment\n    return {'sentiment': sentiment_score, 'stage': stage, 'emoji': emoji} \n\n# ============================\n# Function to Generate a Response using Gemini API with Context\n# ============================\ndef generate_response_with_context(input_message, retrieved_context, conversation_history):\n    history_text = \"\\n\".join(conversation_history)\n    \n    # Ensure retrieved_context is a list of strings and join them into a single string\n    if isinstance(retrieved_context, list):\n        retrieved_context = \"\\n\".join(retrieved_context)\n    \n    # Combine the retrieved context from Pinecone and the local history\n    combined_context = retrieved_context + \"\\n\" + history_text\n    \n    # Analyze the sentiment of the input message\n    sentiment_info = analyze_conversation(input_message)\n    sentiment_emoji = sentiment_info['emoji']\n    \n    prompt = (\n        \"You are a conversational assistant keep normal english and balance length, helping the user improve their sentences must based on the current context of their conversation. \"\n        \"Your task must to generate three distinct suggestions for improving the next sentence the user intends to send. \"\n        \"Each suggestion mandatory should be based on the emotional tone, level of interest, and clarity of the user's current sentence. \"\n        \"Consider the must user's feelings, relationship dynamics, and conversational flow when suggesting improvements.\\n\\n\"\n    )\n    prompt += \"Combined Context:\\n\" + combined_context + \"\\n\\n\"\n    prompt += \"New Message (User's current sentence): \" + input_message + \"\\n\\n\"\n    prompt += f\"Sentiment: {sentiment_info['stage']} {sentiment_emoji}\\n\\n\"\n    prompt += \"Generate three distinct suggestions to improve this sentence, taking into account emotional tone, user interest, and clarity:\\nOption 1:\"\n    \n    data = {\n        \"contents\": [{\n            \"parts\": [{\"text\": prompt}]\n        }]\n    }\n    headers = {\"Content-Type\": \"application/json\"}\n    try:\n        response = requests.post(f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={YOUR_GEMINI_API_KEY}\", headers=headers, json=data)\n        if response.status_code == 200:\n            response_data = response.json()\n            # Check if there are enough suggestions\n            num_suggestions = len(response_data['candidates'][0]['content']['parts'])\n\n            # Collect all available suggestions\n            suggestions = []\n            for i in range(min(3, num_suggestions)):  # Ensure we don't go out of range\n                suggestion = response_data['candidates'][0]['content']['parts'][i]['text']\n                suggestions.append(suggestion)\n            \n            return \"\\n\".join(suggestions)\n        else:\n            print(f\"Gemini API request failed with status code {response.status_code}\")\n            return \"I'm sorry, I cannot generate a response right now.\"\n    except Exception as e:\n        print(\"Error generating response with Gemini:\", e)\n        return \"I'm sorry, I cannot generate a response right now.\"\n\n# ============================\n# Main Chat Loop\n# ============================\nconversation_history = []  # Initialize the conversation history at the start\n\ndef chat_loop():\n    global conversation_history\n    print(\"Start chatting (type 'exit' to end)\")\n    while True:\n        # --- Boy's Turn ---\n        boy_input = input(\"Boy: \")\n        if boy_input.lower() == 'exit':\n            break\n        conversation_history.append(f\"Boy: {boy_input}\")\n        \n        # Retrieve relevant context from Pinecone using Boy's input\n        retrieved_context = vectorstore.similarity_search(boy_input, k=1, namespace=NAMESPACE)\n        \n        # Generate response options for Boy\n        generated_options = generate_response_with_context(boy_input, retrieved_context, conversation_history)\n        print(\"Response Options for Girl:\")\n        print(generated_options)\n        \n        # --- Girl's Turn ---\n        girl_input = input(\"Girl: \")\n        if girl_input.lower() == 'exit':\n            break\n        conversation_history.append(f\"Girl: {girl_input}\")\n        \n        # Retrieve relevant context from Pinecone using Girl's input\n        retrieved_context = vectorstore.similarity_search(girl_input, k=1, namespace=NAMESPACE)\n        \n        # Generate response options for Girl\n        generated_options = generate_response_with_context(girl_input, retrieved_context, conversation_history)\n        print(\"Response Options for Boy:\")\n        print(generated_options)\n        \n    print(\"\\nFinal Conversation History:\")\n    print(\"\\n\".join(conversation_history))\n\nif __name__ == \"__main__\":\n    chat_loop()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:35.388174Z","iopub.execute_input":"2025-04-11T11:42:35.388475Z","iopub.status.idle":"2025-04-11T11:42:43.057741Z","shell.execute_reply.started":"2025-04-11T11:42:35.388426Z","shell.execute_reply":"2025-04-11T11:42:43.056730Z"}},"outputs":[{"name":"stdout","text":"Index 'rchatbot' already exists.\nStart chatting (type 'exit' to end)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Boy:  exit\n"},{"name":"stdout","text":"\nFinal Conversation History:\n\n","output_type":"stream"}],"execution_count":100},{"cell_type":"markdown","source":"creates one with the appropriate dimension and cosine similarity metric for vector-based queries.","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport requests\nimport json\nfrom datetime import datetime\nfrom textblob import TextBlob  # For sentiment analysis\n# Use the Pinecone GRPC client\nfrom pinecone.grpc import PineconeGRPC as Pinecone\nfrom pinecone import ServerlessSpec\nfrom langchain.embeddings import SentenceTransformerEmbeddings\n# ============================\n# Initialize Conversation History\n# ============================\nconversation_history = []\n\n# ============================\n# Function to Analyze Sentiment of Conversation\n# ============================\ndef analyze_conversation(text):\n    analysis = TextBlob(text)\n    sentiment_score = analysis.sentiment.polarity\n    if sentiment_score > 0.5:\n        stage = \"positive/developing\"\n        emoji = \"😊\"  # Happy emoji for positive sentiment\n    elif sentiment_score > 0:\n        stage = \"neutral/initial\"\n        emoji = \"🙂\"  # Neutral emoji\n    else:\n        stage = \"negative/needs attention\"\n        emoji = \"😟\"  # Sad emoji for negative sentiment\n    return {'sentiment': sentiment_score, 'stage': stage, 'emoji': emoji}\n\n# ============================\n# Function to Store a Conversation in Pinecone\n# ============================\ndef store_conversation(query, response, namespace):\n    vector = {\n        \"id\": str(datetime.now().timestamp()),\n        \"values\": embeddings.embed_query(f\"{query} {response}\"),\n        \"metadata\": {\n            \"query\": query,\n            \"response\": response,\n            \"timestamp\": datetime.now().isoformat()\n        }\n    }\n    try:\n        index.upsert(vectors=[vector], namespace=namespace)\n        print(\"Conversation stored successfully in Pinecone.\")\n    except Exception as e:\n        print(\"Error storing conversation in Pinecone:\", e)\n\n# ============================\n# Function to Query Similar Conversations from Pinecone\n# ============================\ndef get_similar_conversation(input_text, namespace):\n    if \":\" in input_text:\n        cleaned_input = input_text.split(\":\", 1)[1].strip()\n    else:\n        cleaned_input = input_text\n    query_embedding = embeddings.embed_query(cleaned_input)\n    try:\n        results = index.query(\n            vector=query_embedding,\n            top_k=1,  # Retrieve only one top match\n            include_metadata=True,\n            namespace=namespace\n        )\n    except Exception as e:\n        print(\"Error querying Pinecone:\", e)\n        return \"No similar past conversations found.\"\n    \n    retrieved_contexts = []\n    for match in results.get('matches', []):\n        metadata = match.get('metadata', {})\n        if \"query\" in metadata and \"response\" in metadata:\n            context = f\"Q: {metadata['query']}\\nA: {metadata['response']}\"\n            retrieved_contexts.append(context)\n    if retrieved_contexts:\n        return \"\\n\".join(retrieved_contexts)\n    return \"No similar past conversations found.\"\n\n# ============================\n# Function to Generate a Response using Gemini API with Context\n# ============================\ndef generate_response_with_context(input_message, retrieved_context, conversation_history):\n    history_text = \"\\n\".join(conversation_history)\n    \n    # Analyze the sentiment of the input message\n    sentiment_info = analyze_conversation(input_message)\n    sentiment_emoji = sentiment_info['emoji']\n    prompt = (\n        \"You are a conversational assistant as normal english with balance length, helping the user improve their sentences based on the context of their conversation. \"\n        \"Your task is to generate three distinct suggestions for improving the next sentence the user intends to send. \"\n        \"Each suggestion should be based on the emotional tone, level of interest, and clarity of the user's current sentence. \"\n        \"Consider the user's feelings, relationship dynamics, and conversational flow when suggesting improvements.\\n\\n\"\n    )\n    prompt += \"Retrieved Past Conversations:\\n\" + retrieved_context + \"\\n\\n\"\n    prompt += \"Current Conversation History:\\n\" + history_text + \"\\n\\n\"\n    prompt += \"New Message (User's current sentence): \" + input_message + \"\\n\\n\"\n    prompt += f\"Sentiment: {sentiment_info['stage']} {sentiment_emoji}\\n\\n\"\n    prompt += \"Generate three distinct suggestions to improve this sentence, taking into account emotional tone, user interest, and clarity:\\nOption 1:\"\n    \n    data = {\n        \"contents\": [{\n            \"parts\": [{\"text\": prompt}]\n        }]\n    }\n    headers = {\"Content-Type\": \"application/json\"}\n    try:\n        response = requests.post(GEMINI_API_URL, headers=headers, json=data)\n        if response.status_code == 200:\n            response_data = response.json()\n            return response_data['candidates'][0]['content']['parts'][0]['text']\n        else:\n            print(f\"Gemini API request failed with status code {response.status_code}\")\n            return \"I'm sorry, I cannot generate a response right now.\"\n    except Exception as e:\n        print(\"Error generating response with Gemini:\", e)\n        return \"I'm sorry, I cannot generate a response right now.\"\n\n# ============================\n# Main Chat Loop\n# ============================\ndef chat_loop():\n    global conversation_history\n    print(\"Start chatting (type 'exit' to end)\")\n    \n    while True:\n        # --- Boy's Turn ---\n        boy_input = input(\"Boy: \")\n        if boy_input.lower() == 'exit':\n            break\n        conversation_history.append(f\"Boy: {boy_input}\")\n        \n        # Retrieve relevant context from Pinecone using Boy's input\n        retrieved_context = get_similar_conversation(f\"Boy: {boy_input}\", NAMESPACE)\n        \n        # Generate response options for Boy\n        generated_options = generate_response_with_context(boy_input, retrieved_context, conversation_history)\n        print(\"Response Options for Girl:\")\n        print(generated_options)\n        \n        # --- Girl's Turn ---\n        girl_input = input(\"Girl: \")\n        if girl_input.lower() == 'exit':\n            break\n        conversation_history.append(f\"Girl: {girl_input}\")\n        \n        # Retrieve relevant context from Pinecone using Girl's input\n        retrieved_context = get_similar_conversation(f\"Girl: {girl_input}\", NAMESPACE)\n        \n        # Generate response options for Girl\n        generated_options = generate_response_with_context(girl_input, retrieved_context, conversation_history)\n        print(\"Response Options for Boy:\")\n        print(generated_options)\n        \n    print(\"\\nFinal Conversation History:\")\n    print(\"\\n\".join(conversation_history))\n\nif __name__ == \"__main__\":\n    chat_loop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:43.058839Z","iopub.execute_input":"2025-04-11T11:42:43.059115Z","iopub.status.idle":"2025-04-11T11:42:46.198867Z","shell.execute_reply.started":"2025-04-11T11:42:43.059092Z","shell.execute_reply":"2025-04-11T11:42:46.197856Z"}},"outputs":[{"name":"stdout","text":"Start chatting (type 'exit' to end)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Boy:  exit\n"},{"name":"stdout","text":"\nFinal Conversation History:\n\n","output_type":"stream"}],"execution_count":101},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport time\nimport requests\nimport json\nfrom datetime import datetime\n# Use the Pinecone GRPC client\nfrom pinecone.grpc import PineconeGRPC as Pinecone\nfrom pinecone import ServerlessSpec\nfrom langchain.embeddings import SentenceTransformerEmbeddings\n\n# # ============================\n# # Environment & API Key Setup\n# # ============================\n# os.environ[\"PINECONE_API_KEY\"] = \"pcsk_vHf69_7u1ntwcfZtKaJvv6fyiMa4LEERHthZ7doGtT2qJei8TAw3acJysg3do6sjdz34Z\"\n# os.environ[\"PINECONE_ENVIRONMENT\"] = \"us-east-1\"\n# os.environ[\"PINECONE_INDEX_NAME\"] = \"rchatbot\"\n\n# YOUR_GEMINI_API_KEY = 'AIzaSyCKPn2qhjs4HGdWoYVuXnt8ssfDlFwGFbc'\n# PINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\n# NAMESPACE = \"relationship_chat\"\n# index_name = os.environ[\"PINECONE_INDEX_NAME\"]\n\n# # ============================\n# # Initialize Pinecone and Index using PineconeGRPC\n# # ============================\n# pc = Pinecone(api_key=PINECONE_API_KEY)\n# if index_name not in pc.list_indexes().names():\n#     print(f\"Index '{index_name}' not found, creating it...\")\n#     pc.create_index(\n#         name=index_name,\n#         dimension=384,  # Must match your embedding model's dimension\n#         metric=\"cosine\",\n#         spec=ServerlessSpec(\n#             cloud=\"aws\",\n#             region=os.environ[\"PINECONE_ENVIRONMENT\"]\n#         )\n#     )\n#     print(f\"Index '{index_name}' created successfully.\")\n# else:\n#     print(f\"Index '{index_name}' already exists.\")\n\n# index = pc.Index(index_name)\n\n# ============================\n# Gemini API Setup (REST call)\n# ============================\nGEMINI_API_URL = f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={YOUR_GEMINI_API_KEY}\"\n\n# ============================\n# Initialize Embeddings\n# ============================\nembeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n# ============================\n# Initialize Conversation History\n# ============================\n# We'll track the conversation manually in a list.\nconversation_history = []\n\n# ============================\n# Function to Store a Conversation in Pinecone\n# ============================\ndef store_conversation(query, response, namespace):\n    vector = {\n        \"id\": str(datetime.now().timestamp()),\n        \"values\": embeddings.embed_query(f\"{query} {response}\"),\n        \"metadata\": {\n            \"query\": query,\n            \"response\": response,\n            \"timestamp\": datetime.now().isoformat()\n        }\n    }\n    try:\n        index.upsert(vectors=[vector], namespace=namespace)\n        print(\"Conversation stored successfully in Pinecone.\")\n    except Exception as e:\n        print(\"Error storing conversation in Pinecone:\", e)\n\n# ============================\n# Function to Query Similar Conversations from Pinecone\n# ============================\ndef get_similar_conversation(input_text, namespace):\n    # Remove speaker label for a cleaner query\n    if \":\" in input_text:\n        cleaned_input = input_text.split(\":\", 1)[1].strip()\n    else:\n        cleaned_input = input_text\n    query_embedding = embeddings.embed_query(cleaned_input)\n    try:\n        results = index.query(\n            vector=query_embedding,\n            top_k=1,  # Retrieve only one top matchPinecone's .query() method is essentially performing a similarity search, helping you find the most relevant vectors in the index based on how similar they are to the input query vector.\n            include_metadata=True,\n            namespace=namespace\n        )\n    except Exception as e:\n        print(\"Error querying Pinecone:\", e)\n        return \"No similar past conversations found.\"\n    \n    retrieved_contexts = []\n    for match in results.get('matches', []):\n        metadata = match.get('metadata', {})\n        if \"query\" in metadata and \"response\" in metadata:\n            context = f\"Q: {metadata['query']}\\nA: {metadata['response']}\"\n            retrieved_contexts.append(context)\n    if retrieved_contexts:\n        return \"\\n\".join(retrieved_contexts)\n    return \"No similar past conversations found.\"\n\n# ============================\n# Function to Generate a Response using Gemini API with Context\n# ============================\ndef generate_response_with_context(input_message, retrieved_context, conversation_history):\n    # Join the manually maintained conversation history\n    history_text = \"\\n\".join(conversation_history)\n    prompt = (\n        \"You are a relationship counselor must balance generated text length . Using the retrieved past conversation context and the current conversation history, \"\n        \"generate three distinct response options for what to say next. Each option should be clearly labeled as Option 1, Option 2, and Option 3.\\n\\n\"\n    )\n    prompt += \"Retrieved Past Conversations:\\n\" + retrieved_context + \"\\n\\n\"\n    prompt += \"Current Conversation History:\\n\" + history_text + \"\\n\\n\"\n    prompt += \"New Message: \" + input_message + \"\\n\\nGenerate three response options:\\nOption 1:\"\n    \n    data = {\n        \"contents\": [{\n            \"parts\": [{\"text\": prompt}]\n        }]\n    }\n    headers = {\"Content-Type\": \"application/json\"}\n    try:\n        response = requests.post(GEMINI_API_URL, headers=headers, json=data)\n        if response.status_code == 200:\n            response_data = response.json()\n            return response_data['candidates'][0]['content']['parts'][0]['text']\n        else:\n            print(f\"Gemini API request failed with status code {response.status_code}\")\n            return \"I'm sorry, I cannot generate a response right now.\"\n    except Exception as e:\n        print(\"Error generating response with Gemini:\", e)\n        return \"I'm sorry, I cannot generate a response right now.\"\n\n# ============================\n# Main Chat Loop\n# ============================\ndef chat_loop():\n    global conversation_history\n    print(\"Start chatting (type 'exit' to end)\")\n    \n    while True:\n        # --- Boy's Turn ---\n        boy_input = input(\"Boy: \")\n        if boy_input.lower() == 'exit':\n            break\n        conversation_history.append(f\"Boy: {boy_input}\")\n        \n        # Retrieve relevant context from Pinecone using Boy's input\n        retrieved_context = get_similar_conversation(f\"Boy: {boy_input}\", NAMESPACE)\n        \n        # Generate response options for Boy\n        generated_options = generate_response_with_context(boy_input, retrieved_context, conversation_history)\n        print(\"Response Options for Boy:\")\n        print(generated_options)\n        \n        # Optionally, you can store the conversation pair (if a response is chosen)\n        # store_conversation(boy_input, <selected_response>, NAMESPACE)\n        \n        # --- Girl's Turn ---\n        girl_input = input(\"Girl: \")\n        if girl_input.lower() == 'exit':\n            break\n        conversation_history.append(f\"Girl: {girl_input}\")\n        \n        # Retrieve relevant context from Pinecone using Girl's input\n        retrieved_context = get_similar_conversation(f\"Girl: {girl_input}\", NAMESPACE)\n        \n        # Generate response options for Girl\n        generated_options = generate_response_with_context(girl_input, retrieved_context, conversation_history)\n        print(\"Response Options for Girl:\")\n        print(generated_options)\n        # Optionally, store the conversation pair as needed\n        \n    print(\"\\nFinal Conversation History:\")\n    print(\"\\n\".join(conversation_history))\n\nif __name__ == \"__main__\":\n    chat_loop()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:46.199956Z","iopub.execute_input":"2025-04-11T11:42:46.200341Z","iopub.status.idle":"2025-04-11T11:42:49.800332Z","shell.execute_reply.started":"2025-04-11T11:42:46.200301Z","shell.execute_reply":"2025-04-11T11:42:49.798894Z"}},"outputs":[{"name":"stdout","text":"Start chatting (type 'exit' to end)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Boy:  exit\n"},{"name":"stdout","text":"\nFinal Conversation History:\n\n","output_type":"stream"}],"execution_count":102},{"cell_type":"markdown","source":"<!-- Full Conversation History:\nBoy: Hello\nGirl: Hii! What's been going on with you lately?\nBoy: How was your day?\nGirl: It was great, thanks for asking!\nBoy: I'm feeling down today.\nGirl: Why do you feel that way?\nBoy: Do you think we are in a serious relationship?\"\nGirl: I don’t know, I’m not sure yet.\nBoy: I'm really sorry about earlier.\nGirl: It's okay, don’t worry about it.\nBoy: I think I’m falling for you\nGirl: Aww, that’s sweet.\nBoy: What do you enjoy doing in your free time?\nGirl: I love reading books and watching movies.\nBoy: Where do you see us in 5 years?\nGirl: \"I haven’t really thought about that yet.\"\nBoy: I’m not feeling great today.\nGirl: I understand, want to talk about it?\nBoy: You look amazing today.\" -->","metadata":{}},{"cell_type":"markdown","source":"Full Conversation History:\nBoy: Hello\nGirl: Hii! What's been going on with you lately?\nBoy: How was your day?\nGirl: It was great, thanks for asking!\nBoy: I'm feeling down today.\nGirl: Why do you feel that way?\nBoy: Do you think we are in a serious relationship?\"\nGirl: I don’t know, I’m not sure yet.\nBoy: I'm really sorry about earlier.\nGirl: It's okay, don’t worry about it.\nBoy: I think I’m falling for you\nGirl: Aww, that’s sweet.\nBoy: What do you enjoy doing in your free time?\nGirl: I love reading books and watching movies.\nBoy: Where do you see us in 5 years?\nGirl: \"I haven’t really thought about that yet.\"\nBoy: I’m not feeling great today.\nGirl: I understand, want to talk about it?\nBoy: You look amazing today.\"","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport requests\nimport json\nfrom datetime import datetime\n# Use the Pinecone GRPC client\nfrom pinecone.grpc import PineconeGRPC as Pinecone\nfrom pinecone import ServerlessSpec\nfrom langchain.embeddings import SentenceTransformerEmbeddings\n\n\n# ============================\n# Initialize Conversation History\n# ============================\nconversation_history = []\n\n# ============================\n# Function to Store a Conversation in Pinecone\n# ============================\ndef store_conversation(query, response, namespace):\n    vector = {\n        \"id\": str(datetime.now().timestamp()),\n        \"values\": embeddings.embed_query(f\"{query} {response}\"),\n        \"metadata\": {\n            \"query\": query,\n            \"response\": response,\n            \"timestamp\": datetime.now().isoformat()\n        }\n    }\n    try:\n        index.upsert(vectors=[vector], namespace=namespace)\n        print(\"Conversation stored successfully in Pinecone.\")\n    except Exception as e:\n        print(\"Error storing conversation in Pinecone:\", e)\n\n# ============================\n# Function to Query Similar Conversations from Pinecone\n# ============================\ndef get_similar_conversation(input_text, namespace):\n    if \":\" in input_text:\n        cleaned_input = input_text.split(\":\", 1)[1].strip()\n    else:\n        cleaned_input = input_text\n    query_embedding = embeddings.embed_query(cleaned_input)\n    try:\n        results = index.query(\n            vector=query_embedding,\n            top_k=1,  # Retrieve only one top match\n            include_metadata=True,\n            namespace=namespace\n        )\n    except Exception as e:\n        print(\"Error querying Pinecone:\", e)\n        return \"No similar past conversations found.\"\n    \n    retrieved_contexts = []\n    for match in results.get('matches', []):\n        metadata = match.get('metadata', {})\n        if \"query\" in metadata and \"response\" in metadata:\n            context = f\"Q: {metadata['query']}\\nA: {metadata['response']}\"\n            retrieved_contexts.append(context)\n    if retrieved_contexts:\n        return \"\\n\".join(retrieved_contexts)\n    return \"No similar past conversations found.\"\n\n# ============================\n# Function to Generate a Response using Gemini API with Context\n# ============================\ndef generate_response_with_context(input_message, retrieved_context, conversation_history):\n    history_text = \"\\n\".join(conversation_history)\n    prompt = (\n        \"You are a relationship counselor. Using the retrieved past conversation context and the current conversation history, \"\n        \"generate three distinct response options for what to say next. Each option should be clearly labeled as Option 1, Option 2, and Option 3.\\n\\n\"\n    )\n    prompt += \"Retrieved Past Conversations:\\n\" + retrieved_context + \"\\n\\n\"\n    prompt += \"Current Conversation History:\\n\" + history_text + \"\\n\\n\"\n    prompt += \"New Message: \" + input_message + \"\\n\\nGenerate three response options:\\nOption 1:\"\n    \n    data = {\n        \"contents\": [{\n            \"parts\": [{\"text\": prompt}]\n        }]\n    }\n    headers = {\"Content-Type\": \"application/json\"}\n    try:\n        response = requests.post(GEMINI_API_URL, headers=headers, json=data)\n        if response.status_code == 200:\n            response_data = response.json()\n            return response_data['candidates'][0]['content']['parts'][0]['text']\n        else:\n            print(f\"Gemini API request failed with status code {response.status_code}\")\n            return \"I'm sorry, I cannot generate a response right now.\"\n    except Exception as e:\n        print(\"Error generating response with Gemini:\", e)\n        return \"I'm sorry, I cannot generate a response right now.\"\n\n# ============================\n# Main Chat Loop\n# ============================\ndef chat_loop():\n    global conversation_history\n    print(\"Start chatting (type 'exit' to end)\")\n    \n    while True:\n        # --- Boy's Turn ---\n        boy_input = input(\"Boy: \")\n        if boy_input.lower() == 'exit':\n            break\n        conversation_history.append(f\"Boy: {boy_input}\")\n        \n        # Retrieve relevant context from Pinecone using Boy's input\n        retrieved_context = get_similar_conversation(f\"Boy: {boy_input}\", NAMESPACE)\n        \n        # Generate response options for Boy\n        generated_options = generate_response_with_context(boy_input, retrieved_context, conversation_history)\n        print(\"Response Options for Girl:\")\n        print(generated_options)\n        \n        # --- Girl's Turn ---\n        girl_input = input(\"Girl: \")\n        if girl_input.lower() == 'exit':\n            break\n        conversation_history.append(f\"Girl: {girl_input}\")\n        \n        # Retrieve relevant context from Pinecone using Girl's input\n        retrieved_context = get_similar_conversation(f\"Girl: {girl_input}\", NAMESPACE)\n        \n        # Generate response options for Girl\n        generated_options = generate_response_with_context(girl_input, retrieved_context, conversation_history)\n        print(\"Response Options for Girl:\")\n        print(generated_options)\n        \n    print(\"\\nFinal Conversation History:\")\n    print(\"\\n\".join(conversation_history))\n\nif __name__ == \"__main__\":\n    chat_loop()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:49.801708Z","iopub.execute_input":"2025-04-11T11:42:49.802192Z","iopub.status.idle":"2025-04-11T11:42:52.881064Z","shell.execute_reply.started":"2025-04-11T11:42:49.802146Z","shell.execute_reply":"2025-04-11T11:42:52.879792Z"}},"outputs":[{"name":"stdout","text":"Start chatting (type 'exit' to end)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Boy:  exit\n"},{"name":"stdout","text":"\nFinal Conversation History:\n\n","output_type":"stream"}],"execution_count":103},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"just need to analyse what function define where calling and what logic they implemented is it right implementation or we can improve then you able to debgging better  where doing wrong and able to design logic for future problem","metadata":{}},{"cell_type":"markdown","source":" According to the documentation, when querying Pinecone, you should pass a single vector rather than a list of queries","metadata":{}},{"cell_type":"markdown","source":"Suggested Responses: For each user input, two suggestions are generated:\n\nModel Generated Response: A response generated by a large language model (e.g., Gemini).\n\nPinecone-based Similar Conversation Response: A response fetched by querying Pinecone for similar conversations based on the embeddings of the input.\n\nEmbedding-Based Search: For each query, you will compare it against previously stored conversations (represented as embeddings) and return the most contextually relevant response.\n\nLet's break down the logic into steps and then provide the updated code.\n\nSteps:\nInput from the user: Get the input message from the user (either the Boy or Girl).\n\nGenerate Suggestions:\n\nFirst Suggestion: Use the LLM (Gemini model) to generate a friendly and contextually appropriate response.\n\nSecond Suggestion: Query Pinecone for similar conversations stored earlier, and return the response that best matches the input using embeddings.\n\nStore the Conversation: Store the input message, the generated response, and other relevant metadata in Pinecone.\n\nReturn the Suggestions: Provide the user with both the LLM-generated response and the Pinecone-retrieved response.","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"https://docs.pinecone.io/integrations/langchain#key-concepts","metadata":{}},{"cell_type":"markdown","source":"******following code for current conversation imporvement ******","metadata":{}},{"cell_type":"code","source":"import os\nimport time\nimport requests\nimport json\nfrom datetime import datetime\nfrom textblob import TextBlob  # For sentiment analysis\nimport pinecone\nfrom pinecone.grpc import PineconeGRPC as Pinecone\nfrom pinecone import ServerlessSpec\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# ============================\n# Environment & API Key Setup\n# ============================\nos.environ[\"PINECONE_API_KEY\"] = \"pcsk_5TbFRd_7Md7Sd3xFm1GFg5GysvwfdHFr11MHWPuHGSHKrahYSTMAufwAWXQxSFLhDtN6RT\"\nos.environ[\"PINECONE_ENVIRONMENT\"] = \"us-east-1\"\nos.environ[\"PINECONE_INDEX_NAME\"] = \"cstagecover\"\n\nYOUR_GEMINI_API_KEY = 'AIzaSyCKPn2qhjs4HGdWoYVuXnt8ssfDlFwGFbc'\nPINECONE_API_KEY = os.environ[\"PINECONE_API_KEY\"]\nNAMESPACE = \"relationship_conver\"\n\n# ============================\n# Initialize Pinecone and Index\n# ============================\npc = Pinecone(api_key=PINECONE_API_KEY)\nindex_name = os.environ[\"PINECONE_INDEX_NAME\"]\n\nif index_name not in pc.list_indexes().names():\n    print(f\"Index '{index_name}' not found, creating it...\")\n    pc.create_index(\n        name=index_name,\n        dimension=384,\n        metric='cosine',\n        spec=ServerlessSpec(cloud='aws', region=os.environ[\"PINECONE_ENVIRONMENT\"])\n    )\n    print(f\"Index '{index_name}' created successfully.\")\nelse:\n    print(f\"Index '{index_name}' already exists.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:52.882108Z","iopub.execute_input":"2025-04-11T11:42:52.882385Z","iopub.status.idle":"2025-04-11T11:42:55.346066Z","shell.execute_reply.started":"2025-04-11T11:42:52.882361Z","shell.execute_reply":"2025-04-11T11:42:55.345166Z"}},"outputs":[{"name":"stdout","text":"Index 'cstagecover' already exists.\n","output_type":"stream"}],"execution_count":104},{"cell_type":"code","source":"import os\nimport time\nimport requests\nimport json\nfrom datetime import datetime\nfrom textblob import TextBlob  # For sentiment analysis\nimport pinecone\nfrom langchain.embeddings import SentenceTransformerEmbeddings\nfrom langchain_pinecone import PineconeVectorStore\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\nindex = pc.Index(index_name)\n\n# ============================\n# Initialize the Embedding Model and Pinecone VectorStore\n# ============================\nembeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n\n# Load the documents and create PineconeVectorStore\n#docs = []  # Load your documents or past conversations here\nvectorstore = PineconeVectorStore.from_documents(docs, embeddings, index_name=index_name)\n\n# ============================\n# Sentiment Analysis Function\n# ============================\ndef analyze_conversation(text):\n    analysis = TextBlob(text)\n    sentiment_score = analysis.sentiment.polarity\n    if sentiment_score > 0.5:\n        stage = \"positive/developing\"\n        emoji = \"😊\"  # Happy emoji for positive sentiment\n    elif sentiment_score > 0:\n        stage = \"neutral/initial\"\n        emoji = \"🙂\"  # Neutral emoji\n    else: \n        stage = \"negative/needs attention\"\n        emoji = \"😟\"  # Sad emoji for negative sentiment\n    return {'sentiment': sentiment_score, 'stage': stage, 'emoji': emoji} \n\n# ============================\n# Function to Generate a Response using Gemini API with Context\n# ============================\ndef generate_response_with_context(input_message, retrieved_context, conversation_history):\n    history_text = \"\\n\".join(conversation_history)\n    \n    # Ensure retrieved_context is a list of strings and join them into a single string\n    if isinstance(retrieved_context, list):\n        retrieved_context = \"\\n\".join(retrieved_context)\n    \n    # Combine the retrieved context from Pinecone and the local history\n    combined_context = retrieved_context + \"\\n\" + history_text\n    \n    # Analyze the sentiment of the input message\n    sentiment_info = analyze_conversation(input_message)\n    sentiment_emoji = sentiment_info['emoji']\n    \n    prompt = (\n        \"You are a conversational assistant helping the user improve their sentence by rephrasing it in a more engaging and emotionally intelligent way. \"\n        \"Your task is to rephrase the current sentence to sound more positive, emotionally engaging, and contextually appropriate. \"\n        \"must should be generate three response\"\n        \"Consider the emotional tone, level of interest, and clarity of the user's current sentence. \"\n        \"Rephrase the following sentence while keeping the same meaning but making it more engaging and appropriate based on the conversation context.\\n\\n\"\n    )\n    \n    prompt += \"Combined Context:\\n\" + combined_context + \"\\n\\n\"\n    prompt += \"Current Message (User's current sentence): \" + input_message + \"\\n\\n\"\n    prompt += f\"Sentiment: {sentiment_info['stage']} {sentiment_emoji}\\n\\n\"\n    prompt += \"Rephrase the above message to improve engagement and emotional tone while maintaining the same meaning:\\nOption 1:\"\n    \n    data = {\n        \"contents\": [{\n            \"parts\": [{\"text\": prompt}]\n        }]\n    }\n    headers = {\"Content-Type\": \"application/json\"}\n    try:\n        response = requests.post(f\"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={YOUR_GEMINI_API_KEY}\", headers=headers, json=data)\n        if response.status_code == 200:\n            response_data = response.json()\n            # Check if there are enough suggestions\n            num_suggestions = len(response_data['candidates'][0]['content']['parts'])\n\n            # Collect all available suggestions\n            suggestions = []\n            for i in range(min(3, num_suggestions)):  # Ensure we don't go out of range\n                suggestion = response_data['candidates'][0]['content']['parts'][i]['text']\n                suggestions.append(suggestion)\n            \n            return \"\\n\".join(suggestions)\n        else:\n            print(f\"Gemini API request failed with status code {response.status_code}\")\n            return \"I'm sorry, I cannot generate a response right now.\"\n    except Exception as e:\n        print(\"Error generating response with Gemini:\", e)\n        return \"I'm sorry, I cannot generate a response right now.\"\n\n\n# ============================\n# Main Chat Loop\n# ============================\nconversation_history = []  # Initialize the conversation history at the start\n\ndef chat_loop():\n    global conversation_history\n    print(\"Start chatting (type 'exit' to end)\")\n    while True:\n        # --- Boy's Turn ---\n        boy_input = input(\"Boy: \")\n        # if boy_input.lower() == 'exit':\n        #     break\n        # conversation_history.append(f\"Boy: {boy_input}\")\n        \n        # Retrieve relevant context from Pinecone using Boy's input\n        retrieved_context = vectorstore.similarity_search(boy_input, k=1, namespace=NAMESPACE)\n        \n        # Generate response options for Boy\n        generated_options = generate_response_with_context(boy_input, retrieved_context, conversation_history)\n        print(\"Response Options for boy:\")\n        print(generated_options)\n        boy_input = input(\"Boy: \")\n        if boy_input.lower() == 'exit':\n            break\n        conversation_history.append(f\"Boy: {boy_input}\")\n        \n        # --- Girl's Turn ---\n        girl_input = input(\"Girl: \")\n        # if girl_input.lower() == 'exit':\n        #     break\n        # conversation_history.append(f\"Girl: {girl_input}\")\n        \n        # Retrieve relevant context from Pinecone using Girl's input\n        retrieved_context = vectorstore.similarity_search(girl_input, k=1, namespace=NAMESPACE)\n        \n        # Generate response options for Girl\n        generated_options = generate_response_with_context(girl_input, retrieved_context, conversation_history)\n        print(\"Response Options for girl:\")\n        print(generated_options)\n        girl_input = input(\"Girl: \")\n        if girl_input.lower() == 'exit':\n             break\n        conversation_history.append(f\"Girl: {girl_input}\")\n        \n    print(\"\\nFinal Conversation History:\")\n    print(\"\\n\".join(conversation_history))\n\nif __name__ == \"__main__\":\n    chat_loop()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-11T11:42:55.347147Z","iopub.execute_input":"2025-04-11T11:42:55.347509Z","iopub.status.idle":"2025-04-11T11:43:08.050589Z","shell.execute_reply.started":"2025-04-11T11:42:55.347470Z","shell.execute_reply":"2025-04-11T11:43:08.049538Z"}},"outputs":[{"name":"stdout","text":"Start chatting (type 'exit' to end)\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Boy:  exit\n"},{"name":"stdout","text":"Response Options for boy:\nOkay, here are three options for rephrasing \"exit\" to be more engaging and emotionally intelligent, considering the sentiment is negative/needs attention:\n\n**Option 1: Acknowledging and Offering Support**\n\n\"Before you go, is there anything I can help you with? I want to make sure everything is resolved to your satisfaction.\" (This option prioritizes understanding *why* the user wants to exit and offers assistance. It shows empathy and a desire to help.)\n\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"Boy:  exit\n"},{"name":"stdout","text":"\nFinal Conversation History:\n\n","output_type":"stream"}],"execution_count":105},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}